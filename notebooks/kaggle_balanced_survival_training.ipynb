{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Restaurant Survival - Balanced Training\n",
    "\n",
    "**Problem**: Extreme imbalance (94.5% success, 5.5% failure)\n",
    "\n",
    "**Solution**: \n",
    "1. Use **stratified sampling** to balance\n",
    "2. Train on top features only (faster)\n",
    "3. Use Gradient Boosting Survival (faster than RSF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q scikit-survival"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import time\n",
    "\n",
    "from sksurv.ensemble import GradientBoostingSurvivalAnalysis\n",
    "from sksurv.metrics import concordance_index_censored\n",
    "from sksurv.util import Surv\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "print(\"‚úÖ Imports complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "DATA_PATH = Path('/kaggle/input/jakarta-restaurant-features-complete')\n",
    "OUTPUT_PATH = Path('/kaggle/working')\n",
    "OUTPUT_PATH.mkdir(exist_ok=True)\n",
    "\n",
    "# Config\n",
    "BALANCE_RATIO = 0.3  # Keep 30% of successes relative to failures\n",
    "TOP_K_FEATURES = 30  # Use top 30 features only\n",
    "\n",
    "GBS_CONFIG = {\n",
    "    'n_estimators': 200,\n",
    "    'learning_rate': 0.1,\n",
    "    'max_depth': 5,\n",
    "    'min_samples_split': 10,\n",
    "    'subsample': 0.8,\n",
    "    'random_state': 42\n",
    "}\n",
    "\n",
    "print(f\"üìÅ Data: {DATA_PATH}\")\n",
    "print(f\"üéØ Balance ratio: {BALANCE_RATIO}\")\n",
    "print(f\"üéØ Top features: {TOP_K_FEATURES}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load\n",
    "df = pd.read_csv(DATA_PATH / 'jakarta_restaurant_features_complete.csv')\n",
    "df_mature = df[df['categorical_label'] != 2].copy()\n",
    "\n",
    "print(f\"‚úÖ Loaded: {len(df_mature):,}\")\n",
    "print(f\"   Failures: {(df_mature['event_observed'] == 1).sum():,}\")\n",
    "print(f\"   Successes: {(df_mature['event_observed'] == 0).sum():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Balance dataset using undersampling\n",
    "failures = df_mature[df_mature['event_observed'] == 1]\n",
    "successes = df_mature[df_mature['event_observed'] == 0]\n",
    "\n",
    "# Sample successes to balance\n",
    "n_failures = len(failures)\n",
    "n_successes_sample = int(n_failures / BALANCE_RATIO)\n",
    "\n",
    "successes_sampled = successes.sample(n=min(n_successes_sample, len(successes)), random_state=42)\n",
    "\n",
    "# Combine\n",
    "df_balanced = pd.concat([failures, successes_sampled], ignore_index=True)\n",
    "df_balanced = df_balanced.sample(frac=1, random_state=42).reset_index(drop=True)  # Shuffle\n",
    "\n",
    "print(f\"\\n‚úÖ Balanced dataset:\")\n",
    "print(f\"   Total: {len(df_balanced):,}\")\n",
    "print(f\"   Failures: {(df_balanced['event_observed'] == 1).sum():,} ({(df_balanced['event_observed'] == 1).mean():.1%})\")\n",
    "print(f\"   Successes: {(df_balanced['event_observed'] == 0).sum():,} ({(df_balanced['event_observed'] == 0).mean():.1%})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get features\n",
    "exclude = ['osm_id', 'name', 'poi_type', 'date_created', 'date_closed',\n",
    "           'survival_days', 'event_observed', 'categorical_label', 'geometry', 'lat', 'lon']\n",
    "feature_cols = [c for c in df_balanced.columns if c not in exclude]\n",
    "\n",
    "# Fill missing\n",
    "df_balanced[feature_cols] = df_balanced[feature_cols].fillna(df_balanced[feature_cols].median())\n",
    "\n",
    "print(f\"‚úÖ Features: {len(feature_cols)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create survival arrays\n",
    "y = Surv.from_arrays(\n",
    "    event=df_balanced['event_observed'].astype(bool),\n",
    "    time=df_balanced['survival_days']\n",
    ")\n",
    "\n",
    "X = df_balanced[feature_cols].values\n",
    "\n",
    "# Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=df_balanced['event_observed']\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Split: Train {len(X_train):,} | Test {len(X_test):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"‚úÖ Scaled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train with ALL Features First"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üî• Training GBS with ALL features (balanced data)...\")\n",
    "start = time.time()\n",
    "\n",
    "gbs_all = GradientBoostingSurvivalAnalysis(**GBS_CONFIG)\n",
    "gbs_all.fit(X_train_scaled, y_train)\n",
    "\n",
    "pred_all = gbs_all.predict(X_test_scaled)\n",
    "c_all = concordance_index_censored(y_test['event'], y_test['time'], pred_all)[0]\n",
    "\n",
    "elapsed = time.time() - start\n",
    "\n",
    "print(f\"\\n‚úÖ Done in {elapsed:.1f}s\")\n",
    "print(f\"   C-index: {c_all:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance\n",
    "importance_df = pd.DataFrame({\n",
    "    'feature': feature_cols,\n",
    "    'importance': gbs_all.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "importance_df['importance_pct'] = importance_df['importance'] / importance_df['importance'].sum() * 100\n",
    "\n",
    "print(\"\\nüìä Top 20 Features:\")\n",
    "print(importance_df.head(20)[['feature', 'importance_pct']].to_string(index=False))\n",
    "\n",
    "importance_df.to_csv(OUTPUT_PATH / 'feature_importance_balanced.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Top-K Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüî• Testing Top-K features...\")\n",
    "\n",
    "k_values = [10, 20, 30, 40, 50]\n",
    "results = []\n",
    "\n",
    "for k in k_values:\n",
    "    print(f\"  K={k}...\", end=\" \")\n",
    "    start = time.time()\n",
    "    \n",
    "    # Select top k\n",
    "    top_k = importance_df.head(k)['feature'].tolist()\n",
    "    k_indices = [feature_cols.index(f) for f in top_k]\n",
    "    \n",
    "    X_train_k = X_train_scaled[:, k_indices]\n",
    "    X_test_k = X_test_scaled[:, k_indices]\n",
    "    \n",
    "    # Train\n",
    "    gbs_k = GradientBoostingSurvivalAnalysis(**GBS_CONFIG)\n",
    "    gbs_k.fit(X_train_k, y_train)\n",
    "    \n",
    "    # Evaluate\n",
    "    pred_k = gbs_k.predict(X_test_k)\n",
    "    c_k = concordance_index_censored(y_test['event'], y_test['time'], pred_k)[0]\n",
    "    \n",
    "    elapsed = time.time() - start\n",
    "    results.append({'k': k, 'c_index': c_k, 'time_s': elapsed})\n",
    "    \n",
    "    print(f\"C-index: {c_k:.4f} ({elapsed:.1f}s)\")\n",
    "\n",
    "df_results = pd.DataFrame(results)\n",
    "print(\"\\nüìä Results:\")\n",
    "print(df_results.to_string(index=False))\n",
    "\n",
    "df_results.to_csv(OUTPUT_PATH / 'top_k_results_balanced.csv', index=False)\n",
    "\n",
    "best = df_results.loc[df_results['c_index'].idxmax()]\n",
    "print(f\"\\nüèÜ Best: k={int(best['k'])} ‚Üí C-index={best['c_index']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üìù SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\n‚úÖ Balancing:\")\n",
    "print(f\"   Original: 72,082 (5.5% failure)\")\n",
    "print(f\"   Balanced: {len(df_balanced):,} ({(df_balanced['event_observed'] == 1).mean():.1%} failure)\")\n",
    "\n",
    "print(f\"\\n‚úÖ Performance:\")\n",
    "print(f\"   All features: {c_all:.4f}\")\n",
    "print(f\"   Best Top-K: {best['c_index']:.4f} (k={int(best['k'])})\")\n",
    "\n",
    "print(f\"\\n‚úÖ Top 5 Features:\")\n",
    "for i, row in importance_df.head(5).iterrows():\n",
    "    print(f\"   {i+1}. {row['feature']:40s} ({row['importance_pct']:.2f}%)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "if c_all > 0.7:\n",
    "    print(\"‚úÖ SUCCESS: C-index > 0.7 (good performance!)\")\n",
    "elif c_all > 0.6:\n",
    "    print(\"‚ö†Ô∏è  MODERATE: C-index 0.6-0.7 (needs improvement)\")\n",
    "else:\n",
    "    print(\"‚ùå POOR: C-index < 0.6 (model not working well)\")\n",
    "\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
