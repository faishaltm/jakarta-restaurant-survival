{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Importance Analysis (Gradient Boosting)\n",
    "\n",
    "## Goal: Identify Which Features Actually Matter\n",
    "\n",
    "**NOT for accuracy - FOR FEATURE RANKING!**\n",
    "\n",
    "**Strategy:**\n",
    "1. Extract ALL candidate features from raw data\n",
    "2. Train Gradient Boosting model (FAST: 20 trees, 3-5 min)\n",
    "3. Analyze feature importance scores\n",
    "4. Identify top 5-10 features that actually contribute\n",
    "5. Discard the rest\n",
    "\n",
    "**Features to Extract:**\n",
    "1. Competitor count at 1km, 2km, 5km\n",
    "2. Nearest competitor distance\n",
    "3. Anchor POI counts (mall, office, transport)\n",
    "4. Income level by district\n",
    "5. District density\n",
    "6. Demand-supply ratio\n",
    "7. Residential/school/hospital counts\n",
    "\n",
    "**Output:**\n",
    "- Feature importance ranking (sorted CSV)\n",
    "- Top 10 features chart\n",
    "- Recommendation: which features to keep for final model\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q scikit-survival\n",
    "print(\"✓ Installation complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "from datetime import datetime, timedelta\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sksurv.ensemble import GradientBoostingSurvivalAnalysis\n",
    "from shapely.strtree import STRtree\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"✓ All libraries imported!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target category\n",
    "TARGET_CATEGORY = 'restaurant'\n",
    "\n",
    "# Paths\n",
    "try:\n",
    "    DATASET_PATH = \"/kaggle/input/jakarta-clean-categorized/jakarta_clean_categorized.csv\"\n",
    "    OUTPUT_DIR = Path(\"/kaggle/working\")\n",
    "    print(\"✓ Running on Kaggle\")\n",
    "except:\n",
    "    DATASET_PATH = \"data/jakarta_clean_categorized.csv\"\n",
    "    OUTPUT_DIR = Path(\"outputs\")\n",
    "    OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "    print(\"✓ Running locally\")\n",
    "\n",
    "# Gradient Boosting config (OPTIMIZED FOR SPEED)\n",
    "GB_CONFIG = {\n",
    "    'n_estimators': 20,      # FAST (just for feature importance)\n",
    "    'learning_rate': 0.2,\n",
    "    'max_depth': 3,\n",
    "    'subsample': 0.8,\n",
    "    'random_state': 42,\n",
    "    'verbose': 1\n",
    "}\n",
    "\n",
    "print(f\"\\nTarget: {TARGET_CATEGORY}\")\n",
    "print(f\"Gradient Boosting: {GB_CONFIG['n_estimators']} trees (for feature ranking only)\")\n",
    "print(f\"\\n⚠ NOT optimizing for accuracy - ONLY for feature importance!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading dataset...\")\n",
    "df = pd.read_csv(DATASET_PATH)\n",
    "print(f\"✓ Loaded {len(df):,} POIs total\")\n",
    "\n",
    "# Convert ALL to GeoDataFrame (we need all POIs for context)\n",
    "gdf_all = gpd.GeoDataFrame(\n",
    "    df,\n",
    "    geometry=gpd.points_from_xy(df.longitude, df.latitude),\n",
    "    crs='EPSG:4326'\n",
    ").to_crs(epsg=32748)\n",
    "\n",
    "print(f\"✓ Converted all POIs to UTM (EPSG:32748)\")\n",
    "\n",
    "# Filter target category for analysis\n",
    "gdf_target = gdf_all[gdf_all['poi_type'] == TARGET_CATEGORY].copy()\n",
    "print(f\"✓ Target category: {len(gdf_target):,} {TARGET_CATEGORY}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Survival Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Creating survival labels...\\n\")\n",
    "\n",
    "# Parse dates\n",
    "gdf_target['date_created_parsed'] = pd.to_datetime(gdf_target['date_created'], errors='coerce')\n",
    "gdf_target['date_closed_parsed'] = pd.to_datetime(gdf_target['date_closed'], errors='coerce')\n",
    "\n",
    "REFERENCE_DATE = pd.Timestamp('2024-01-01')\n",
    "OBSERVATION_WINDOW_DAYS = 365 * 3\n",
    "\n",
    "# Event and duration\n",
    "gdf_target['event'] = gdf_target['date_closed_parsed'].notna()\n",
    "gdf_target['duration'] = np.where(\n",
    "    gdf_target['event'],\n",
    "    (gdf_target['date_closed_parsed'] - gdf_target['date_created_parsed']).dt.days,\n",
    "    (REFERENCE_DATE - gdf_target['date_created_parsed']).dt.days\n",
    ")\n",
    "\n",
    "# Categorical label\n",
    "gdf_target['categorical_label'] = 2  # too new\n",
    "gdf_target.loc[\n",
    "    (gdf_target['date_created_parsed'] <= REFERENCE_DATE - timedelta(days=OBSERVATION_WINDOW_DAYS)) & \n",
    "    (gdf_target['date_closed_parsed'].notna()) & \n",
    "    (gdf_target['date_closed_parsed'] <= REFERENCE_DATE),\n",
    "    'categorical_label'\n",
    "] = 0  # failure\n",
    "gdf_target.loc[\n",
    "    (gdf_target['date_created_parsed'] <= REFERENCE_DATE - timedelta(days=OBSERVATION_WINDOW_DAYS)) & \n",
    "    (gdf_target['date_closed_parsed'].isna()),\n",
    "    'categorical_label'\n",
    "] = 1  # success\n",
    "\n",
    "# Filter mature POIs\n",
    "df_mature = gdf_target[gdf_target['categorical_label'] != 2].copy()\n",
    "\n",
    "print(f\"Mature {TARGET_CATEGORY}s: {len(df_mature):,}\")\n",
    "print(f\"  Failures: {(df_mature['categorical_label'] == 0).sum():,}\")\n",
    "print(f\"  Successes: {(df_mature['categorical_label'] == 1).sum():,}\")\n",
    "print(f\"  Failure rate: {(df_mature['categorical_label'] == 0).sum() / len(df_mature) * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction: Competition Features\n",
    "\n",
    "Extract multiple distance scales (1km, 2km, 5km)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"EXTRACTING COMPETITION FEATURES\")\n",
    "print(\"=\"*80)\n",
    "print()\n",
    "\n",
    "# Build spatial index for target category\n",
    "target_tree = STRtree(gdf_target.geometry)\n",
    "\n",
    "BUFFER_SIZES = [1000, 2000, 5000]  # meters\n",
    "\n",
    "for buffer_m in BUFFER_SIZES:\n",
    "    print(f\"\\nExtracting at {buffer_m}m...\")\n",
    "    \n",
    "    counts = []\n",
    "    for idx, poi in tqdm(df_mature.iterrows(), total=len(df_mature), desc=f\"  {buffer_m}m\"):\n",
    "        buffer = poi.geometry.buffer(buffer_m)\n",
    "        nearby_indices = target_tree.query(buffer)\n",
    "        nearby = gdf_target.iloc[nearby_indices]\n",
    "        \n",
    "        # Count competitors (exclude self)\n",
    "        competitors = nearby[nearby.index != idx]\n",
    "        competitors = competitors[competitors.geometry.within(buffer)]\n",
    "        \n",
    "        counts.append(len(competitors))\n",
    "    \n",
    "    col_name = f'competitors_{buffer_m}m'\n",
    "    df_mature[col_name] = counts\n",
    "    print(f\"  ✓ {col_name}: mean={np.mean(counts):.1f}, median={np.median(counts):.0f}\")\n",
    "\n",
    "print(f\"\\n✓ Competition features extracted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction: Nearest Competitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nExtracting: Nearest competitor distance...\")\n",
    "\n",
    "nearest_distances = []\n",
    "for idx, poi in tqdm(df_mature.iterrows(), total=len(df_mature), desc=\"  Nearest competitor\"):\n",
    "    others = gdf_target[gdf_target.index != idx]\n",
    "    if len(others) > 0:\n",
    "        distances = others.geometry.distance(poi.geometry)\n",
    "        nearest_distances.append(distances.min())\n",
    "    else:\n",
    "        nearest_distances.append(10000)\n",
    "\n",
    "df_mature['nearest_competitor_m'] = nearest_distances\n",
    "print(f\"✓ nearest_competitor_m: mean={np.mean(nearest_distances):.0f}m\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction: Anchor POIs (Malls, Offices, Transport)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"EXTRACTING ANCHOR POI FEATURES\")\n",
    "print(\"=\"*80)\n",
    "print()\n",
    "\n",
    "# Define anchor types\n",
    "ANCHOR_TYPES = {\n",
    "    'mall': ['mall'],\n",
    "    'office': ['office'],\n",
    "    'transport': ['transport']\n",
    "}\n",
    "\n",
    "for anchor_name, poi_types in ANCHOR_TYPES.items():\n",
    "    print(f\"\\nExtracting: {anchor_name} counts...\")\n",
    "    \n",
    "    # Filter anchor POIs\n",
    "    gdf_anchor = gdf_all[gdf_all['poi_type'].isin(poi_types)]\n",
    "    print(f\"  {anchor_name} POIs: {len(gdf_anchor):,}\")\n",
    "    \n",
    "    if len(gdf_anchor) == 0:\n",
    "        # No POIs of this type\n",
    "        df_mature[f'{anchor_name}_count_1000m'] = 0\n",
    "        continue\n",
    "    \n",
    "    # Build spatial index\n",
    "    anchor_tree = STRtree(gdf_anchor.geometry)\n",
    "    \n",
    "    counts = []\n",
    "    for idx, poi in tqdm(df_mature.iterrows(), total=len(df_mature), desc=f\"  {anchor_name}\"):\n",
    "        buffer = poi.geometry.buffer(1000)\n",
    "        nearby_indices = anchor_tree.query(buffer)\n",
    "        nearby = gdf_anchor.iloc[nearby_indices]\n",
    "        within = nearby[nearby.geometry.within(buffer)]\n",
    "        counts.append(len(within))\n",
    "    \n",
    "    col_name = f'{anchor_name}_count_1000m'\n",
    "    df_mature[col_name] = counts\n",
    "    print(f\"  ✓ {col_name}: mean={np.mean(counts):.1f}\")\n",
    "\n",
    "print(f\"\\n✓ Anchor features extracted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction: Other POI Categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"EXTRACTING OTHER POI CATEGORY FEATURES\")\n",
    "print(\"=\"*80)\n",
    "print()\n",
    "\n",
    "# Define other relevant categories\n",
    "OTHER_TYPES = ['residential', 'school', 'hospital', 'bank', 'gym', 'university']\n",
    "\n",
    "for poi_type in OTHER_TYPES:\n",
    "    print(f\"\\nExtracting: {poi_type} counts...\")\n",
    "    \n",
    "    # Filter POIs\n",
    "    gdf_type = gdf_all[gdf_all['poi_type'] == poi_type]\n",
    "    print(f\"  {poi_type} POIs: {len(gdf_type):,}\")\n",
    "    \n",
    "    if len(gdf_type) == 0:\n",
    "        df_mature[f'{poi_type}_count_1000m'] = 0\n",
    "        continue\n",
    "    \n",
    "    # Build spatial index\n",
    "    type_tree = STRtree(gdf_type.geometry)\n",
    "    \n",
    "    counts = []\n",
    "    for idx, poi in tqdm(df_mature.iterrows(), total=len(df_mature), desc=f\"  {poi_type}\"):\n",
    "        buffer = poi.geometry.buffer(1000)\n",
    "        nearby_indices = type_tree.query(buffer)\n",
    "        nearby = gdf_type.iloc[nearby_indices]\n",
    "        within = nearby[nearby.geometry.within(buffer)]\n",
    "        counts.append(len(within))\n",
    "    \n",
    "    col_name = f'{poi_type}_count_1000m'\n",
    "    df_mature[col_name] = counts\n",
    "    print(f\"  ✓ {col_name}: mean={np.mean(counts):.1f}\")\n",
    "\n",
    "print(f\"\\n✓ Other POI features extracted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction: Demographics (District-Based)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"EXTRACTING DEMOGRAPHIC FEATURES\")\n",
    "print(\"=\"*80)\n",
    "print()\n",
    "\n",
    "# Income estimates (millions IDR/month)\n",
    "jakarta_income = {\n",
    "    'Setiabudi': 22.8, 'Kebayoran Baru': 18.5, 'Menteng': 19.3,\n",
    "    'Tanah Abang': 12.4, 'Cilandak': 16.2, 'Kebayoran Lama': 14.1,\n",
    "    'Mampang Prapatan': 15.3, 'Tebet': 13.9, 'Pancoran': 11.7,\n",
    "    'Pasar Minggu': 9.8, 'Jagakarsa': 9.5, 'Pesanggrahan': 9.2,\n",
    "    'Gambir': 11.1, 'Kemayoran': 10.3, 'Sawah Besar': 9.1,\n",
    "    'Senen': 8.1, 'Cempaka Putih': 10.8, 'Johar Baru': 8.9,\n",
    "    'Cakung': 7.8, 'Jatinegara': 9.4, 'Kramat Jati': 8.5,\n",
    "    'Matraman': 9.9, 'Pasar Rebo': 8.2, 'Ciracas': 7.9,\n",
    "    'Cengkareng': 8.7, 'Grogol Petamburan': 11.5, 'Kalideres': 7.5,\n",
    "    'Kebon Jeruk': 12.8, 'Kembangan': 8.4, 'Palmerah': 13.2,\n",
    "}\n",
    "\n",
    "# Population density (per km²)\n",
    "jakarta_density = {\n",
    "    'Cilandak': 5979, 'Jagakarsa': 12281, 'Kebayoran Baru': 7999,\n",
    "    'Kebayoran Lama': 9629, 'Mampang Prapatan': 7112, 'Pancoran': 9885,\n",
    "    'Pasar Minggu': 9081, 'Pesanggrahan': 7955, 'Setiabudi': 8572,\n",
    "    'Tebet': 13010, 'Cempaka Putih': 8855, 'Gambir': 5746,\n",
    "    'Johar Baru': 27135, 'Kemayoran': 14957, 'Menteng': 16111,\n",
    "    'Sawah Besar': 27769, 'Senen': 19499, 'Tanah Abang': 17797,\n",
    "    'Cakung': 11466, 'Ciracas': 10203, 'Jatinegara': 24500,\n",
    "    'Kramat Jati': 12178, 'Matraman': 18670, 'Pasar Rebo': 11704,\n",
    "    'Cengkareng': 13897, 'Kalideres': 15811, 'Kebon Jeruk': 12165,\n",
    "    'Kembangan': 17094, 'Palmerah': 25872,\n",
    "}\n",
    "\n",
    "df_mature['income_district_m'] = df_mature['district'].map(\n",
    "    lambda x: jakarta_income.get(str(x).replace(' ', ''), 10.5)\n",
    ")\n",
    "\n",
    "df_mature['density_district'] = df_mature['district'].map(\n",
    "    lambda x: jakarta_density.get(str(x).replace(' ', ''), 12000)\n",
    ")\n",
    "\n",
    "print(f\"✓ income_district_m: mean={df_mature['income_district_m'].mean():.1f}M IDR\")\n",
    "print(f\"✓ density_district: mean={df_mature['density_district'].mean():.0f} per km²\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction: Derived Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"CREATING DERIVED FEATURES\")\n",
    "print(\"=\"*80)\n",
    "print()\n",
    "\n",
    "# Demand-supply ratio\n",
    "df_mature['demand_supply_1km'] = (\n",
    "    df_mature['density_district'] / (df_mature['competitors_1000m'] + 1)\n",
    ")\n",
    "print(f\"✓ demand_supply_1km: mean={df_mature['demand_supply_1km'].mean():.1f}\")\n",
    "\n",
    "# Income × Density interaction\n",
    "df_mature['income_density_interaction'] = (\n",
    "    df_mature['income_district_m'] * df_mature['density_district'] / 1000\n",
    ")\n",
    "print(f\"✓ income_density_interaction: mean={df_mature['income_density_interaction'].mean():.1f}\")\n",
    "\n",
    "# Competition pressure (competitors per 1000 people)\n",
    "df_mature['competition_pressure'] = (\n",
    "    df_mature['competitors_1000m'] * 1000 / (df_mature['density_district'] + 1)\n",
    ")\n",
    "print(f\"✓ competition_pressure: mean={df_mature['competition_pressure'].mean():.1f}\")\n",
    "\n",
    "print(f\"\\n✓ Derived features created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary: All Features Extracted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all feature columns\n",
    "exclude_cols = [\n",
    "    'name', 'latitude', 'longitude', 'date_created', 'date_closed',\n",
    "    'district', 'regency', 'province', 'poi_type', 'source',\n",
    "    'date_created_parsed', 'date_closed_parsed', 'event', 'duration',\n",
    "    'categorical_label', 'geometry'\n",
    "]\n",
    "\n",
    "feature_cols = [col for col in df_mature.columns if col not in exclude_cols]\n",
    "feature_cols = [col for col in feature_cols if df_mature[col].dtype in ['int64', 'float64']]\n",
    "\n",
    "print(f\"=\"*80)\n",
    "print(f\"TOTAL FEATURES EXTRACTED: {len(feature_cols)}\")\n",
    "print(f\"=\"*80)\n",
    "print()\n",
    "print(\"Features:\")\n",
    "for i, feat in enumerate(feature_cols, 1):\n",
    "    print(f\"  {i:2d}. {feat}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Gradient Boosting for Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"TRAINING GRADIENT BOOSTING (FOR FEATURE IMPORTANCE)\")\n",
    "print(\"=\"*80)\n",
    "print()\n",
    "\n",
    "# Prepare data\n",
    "X = df_mature[feature_cols].fillna(0).replace([np.inf, -np.inf], 0).values\n",
    "y = np.array(\n",
    "    list(zip(df_mature['event'], df_mature['duration'])),\n",
    "    dtype=[('event', bool), ('duration', float)]\n",
    ")\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=df_mature['event']\n",
    ")\n",
    "\n",
    "# Class weights\n",
    "n_failures = y_train['event'].sum()\n",
    "n_successes = len(y_train) - n_failures\n",
    "sample_weights = np.where(\n",
    "    y_train['event'],\n",
    "    len(y_train) / (2 * n_failures),\n",
    "    len(y_train) / (2 * n_successes)\n",
    ")\n",
    "\n",
    "print(f\"Training data: {len(X_train):,} samples\")\n",
    "print(f\"Features: {len(feature_cols)}\")\n",
    "print(f\"Model: Gradient Boosting with {GB_CONFIG['n_estimators']} trees\")\n",
    "print(f\"\\n⚠ Training for feature importance ONLY (not accuracy)\\n\")\n",
    "\n",
    "# Train\n",
    "import time\n",
    "gb_model = GradientBoostingSurvivalAnalysis(**GB_CONFIG)\n",
    "\n",
    "start = time.time()\n",
    "gb_model.fit(X_train, y_train, sample_weight=sample_weights)\n",
    "elapsed = time.time() - start\n",
    "\n",
    "print(f\"\\n✓ Training complete: {elapsed:.0f}s ({elapsed/60:.1f} min)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"FEATURE IMPORTANCE RANKING\")\n",
    "print(\"=\"*80)\n",
    "print()\n",
    "\n",
    "# Get feature importance\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': feature_cols,\n",
    "    'importance': gb_model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "# Calculate cumulative importance\n",
    "feature_importance['cumulative_importance'] = feature_importance['importance'].cumsum()\n",
    "feature_importance['importance_pct'] = feature_importance['importance'] * 100\n",
    "feature_importance['cumulative_pct'] = feature_importance['cumulative_importance'] * 100\n",
    "\n",
    "print(\"\\nTop 20 Most Important Features:\\n\")\n",
    "print(feature_importance.head(20)[['feature', 'importance_pct', 'cumulative_pct']].to_string(index=False))\n",
    "\n",
    "# Save full ranking\n",
    "feature_importance.to_csv(OUTPUT_DIR / 'feature_importance_ranking.csv', index=False)\n",
    "print(f\"\\n✓ Full ranking saved: {OUTPUT_DIR / 'feature_importance_ranking.csv'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Top Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot top 15 features\n",
    "plt.figure(figsize=(12, 8))\n",
    "top_15 = feature_importance.head(15)\n",
    "\n",
    "plt.barh(top_15['feature'], top_15['importance_pct'], color='#2E86AB')\n",
    "plt.xlabel('Importance (%)', fontsize=12, fontweight='bold')\n",
    "plt.title(f'Top 15 Features - {TARGET_CATEGORY.capitalize()} Survival\\n(Gradient Boosting Feature Importance)', \n",
    "          fontsize=14, fontweight='bold')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save\n",
    "plt.savefig(OUTPUT_DIR / 'feature_importance_top15.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"✓ Chart saved: {OUTPUT_DIR / 'feature_importance_top15.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"RECOMMENDATIONS\")\n",
    "print(\"=\"*80)\n",
    "print()\n",
    "\n",
    "# Find features that contribute 80% of importance\n",
    "top_80_pct = feature_importance[feature_importance['cumulative_pct'] <= 80]\n",
    "print(f\"Features contributing 80% of importance: {len(top_80_pct)}\")\n",
    "print(f\"\\nThese {len(top_80_pct)} features:\\n\")\n",
    "for idx, row in top_80_pct.iterrows():\n",
    "    print(f\"  • {row['feature']:30s} {row['importance_pct']:5.1f}%\")\n",
    "\n",
    "print(f\"\\n\" + \"=\"*80)\n",
    "print(f\"NEXT STEPS\")\n",
    "print(\"=\"*80)\n",
    "print()\n",
    "print(f\"1. Use ONLY the top {len(top_80_pct)} features for final model\")\n",
    "print(f\"2. Discard the remaining {len(feature_cols) - len(top_80_pct)} features (contribute <20%)\")\n",
    "print(f\"3. Train Random Survival Forest with these {len(top_80_pct)} features\")\n",
    "print(f\"4. Optimize hyperparameters for accuracy\")\n",
    "print()\n",
    "print(f\"Expected benefit:\")\n",
    "print(f\"  • {len(top_80_pct)/len(feature_cols)*100:.0f}% fewer features = faster training\")\n",
    "print(f\"  • Less overfitting risk\")\n",
    "print(f\"  • Same or better accuracy (removed noise)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook extracted ALL candidate features and used Gradient Boosting to identify which ones actually matter.\n",
    "\n",
    "**Output:**\n",
    "- `feature_importance_ranking.csv` - Full ranking of all features\n",
    "- `feature_importance_top15.png` - Visualization of top features\n",
    "- Recommendation: Keep only top N features (80% cumulative importance)\n",
    "\n",
    "**NOT for accuracy - FOR FEATURE SELECTION!**\n",
    "\n",
    "---\n",
    "\n",
    "*Generated with Claude Code*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
