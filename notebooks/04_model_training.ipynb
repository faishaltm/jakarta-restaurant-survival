{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coffee Shop Site Selection - ML Model Training\n",
    "\n",
    "This notebook demonstrates the complete machine learning pipeline:\n",
    "\n",
    "1. **Load training data** (successful coffee shop locations)\n",
    "2. **Generate negative samples** (random locations)\n",
    "3. **Engineer spatial features** (competitor density, POI diversity, etc.)\n",
    "4. **Train Random Forest model** with spatial cross-validation\n",
    "5. **Evaluate performance** and analyze feature importance\n",
    "6. **Save model** for predictions\n",
    "\n",
    "**Target:** 70-80% accuracy with spatial CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add src to path\n",
    "sys.path.append(str(Path.cwd().parent / 'src'))\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from loguru import logger\n",
    "\n",
    "from features.spatial_features import SpatialFeatureEngineer, create_feature_matrix\n",
    "from models.train_model import CoffeeShopModel, generate_negative_samples\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('Set2')\n",
    "\n",
    "print(\"✓ Libraries loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load Training Data\n",
    "\n",
    "Load known successful coffee shop locations (positive samples)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load coffee shop data\n",
    "coffee_path = Path('../data/processed/coffee_shops/jakarta_coffee_shops_training.geojson')\n",
    "\n",
    "if not coffee_path.exists():\n",
    "    print(\"❌ Coffee shop training data not found!\")\n",
    "    print(\"Please run 01_data_collection.ipynb first to collect training data\")\n",
    "    raise FileNotFoundError(f\"Training data not found at {coffee_path}\")\n",
    "\n",
    "positive_samples = gpd.read_file(coffee_path)\n",
    "\n",
    "print(f\"✓ Loaded {len(positive_samples):,} positive training samples\")\n",
    "print(f\"\\nBrands in training data:\")\n",
    "print(positive_samples['brand'].value_counts())\n",
    "\n",
    "positive_samples.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Generate Negative Samples\n",
    "\n",
    "Create negative training samples (random locations that are NOT near existing coffee shops)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jakarta bounding box\n",
    "JAKARTA_BBOX = (106.6, -6.4, 107.1, -6.0)  # min_lon, min_lat, max_lon, max_lat\n",
    "\n",
    "# Generate negative samples (2x the number of positive samples)\n",
    "print(\"Generating negative samples...\")\n",
    "print(\"(Random locations at least 200m away from existing coffee shops)\\n\")\n",
    "\n",
    "negative_samples = generate_negative_samples(\n",
    "    positive_samples=positive_samples,\n",
    "    bbox=JAKARTA_BBOX,\n",
    "    n_samples=len(positive_samples) * 2,\n",
    "    min_distance=200  # meters\n",
    ")\n",
    "\n",
    "print(f\"✓ Generated {len(negative_samples):,} negative samples\")\n",
    "print(f\"\\nClass balance:\")\n",
    "print(f\"  Positive (successful locations): {len(positive_samples)}\")\n",
    "print(f\"  Negative (random locations): {len(negative_samples)}\")\n",
    "print(f\"  Ratio: 1:{len(negative_samples)/len(positive_samples):.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize training samples\n",
    "fig, ax = plt.subplots(figsize=(12, 10))\n",
    "\n",
    "# Plot negative samples\n",
    "negative_samples.plot(\n",
    "    ax=ax,\n",
    "    color='lightblue',\n",
    "    markersize=10,\n",
    "    alpha=0.5,\n",
    "    label='Negative samples (random)'\n",
    ")\n",
    "\n",
    "# Plot positive samples\n",
    "positive_samples.plot(\n",
    "    ax=ax,\n",
    "    color='red',\n",
    "    markersize=20,\n",
    "    alpha=0.7,\n",
    "    label='Positive samples (coffee shops)'\n",
    ")\n",
    "\n",
    "ax.set_title('Training Data: Positive vs Negative Samples', fontsize=16, fontweight='bold')\n",
    "ax.set_xlabel('Longitude')\n",
    "ax.set_ylabel('Latitude')\n",
    "ax.legend(loc='upper right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Engineer Spatial Features\n",
    "\n",
    "Calculate features for both positive and negative samples:\n",
    "- Competitor density (500m, 1km, 2km buffers)\n",
    "- Distance to nearest competitor\n",
    "- POI diversity indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine positive and negative samples for feature engineering\n",
    "all_locations = pd.concat([\n",
    "    positive_samples[['geometry']],\n",
    "    negative_samples[['geometry']]\n",
    "], ignore_index=True)\n",
    "\n",
    "all_locations_gdf = gpd.GeoDataFrame(all_locations, geometry='geometry', crs=positive_samples.crs)\n",
    "\n",
    "print(f\"Total locations for feature engineering: {len(all_locations_gdf):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize feature engineer\n",
    "engineer = SpatialFeatureEngineer(buffer_distances=[500, 1000, 2000])\n",
    "\n",
    "print(\"Calculating spatial features...\")\n",
    "print(\"This may take 5-10 minutes depending on number of samples\\n\")\n",
    "\n",
    "# Calculate features\n",
    "# Using positive_samples as competitors (existing coffee shops)\n",
    "feature_matrix = engineer.calculate_all_buffer_features(\n",
    "    locations=all_locations_gdf,\n",
    "    competitors=positive_samples,\n",
    "    all_pois=None  # Add OSM POIs here if available for diversity calculation\n",
    ")\n",
    "\n",
    "print(f\"\\n✓ Feature engineering complete!\")\n",
    "print(f\"Features calculated: {feature_matrix.shape[1]}\")\n",
    "print(f\"Samples: {feature_matrix.shape[0]}\")\n",
    "\n",
    "print(f\"\\nFeature columns:\")\n",
    "print(feature_matrix.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview features\n",
    "print(\"Feature Statistics:\\n\")\n",
    "print(feature_matrix.describe())\n",
    "\n",
    "feature_matrix.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Prepare Training Data\n",
    "\n",
    "Create labels and split data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create labels\n",
    "y_positive = np.ones(len(positive_samples))\n",
    "y_negative = np.zeros(len(negative_samples))\n",
    "y = np.concatenate([y_positive, y_negative])\n",
    "\n",
    "# Get feature matrix\n",
    "X = feature_matrix.values\n",
    "\n",
    "print(f\"Training data prepared:\")\n",
    "print(f\"  Features (X): {X.shape}\")\n",
    "print(f\"  Labels (y): {y.shape}\")\n",
    "print(f\"  Positive class: {y.sum():.0f} ({y.mean():.1%})\")\n",
    "print(f\"  Negative class: {(1-y).sum():.0f} ({(1-y.mean()):.1%})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Train Model with Spatial Cross-Validation\n",
    "\n",
    "Use spatial CV to prevent overfitting from spatial autocorrelation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "model = CoffeeShopModel(\n",
    "    n_estimators=200,\n",
    "    max_depth=20,\n",
    "    min_samples_split=10,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(\"Model configuration:\")\n",
    "print(f\"  Algorithm: Random Forest\")\n",
    "print(f\"  Trees: 200\")\n",
    "print(f\"  Max depth: 20\")\n",
    "print(f\"  Min samples split: 10\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform spatial cross-validation\n",
    "print(\"\\nPerforming 5-fold spatial cross-validation...\")\n",
    "print(\"(This separates training/test by geographic clusters)\\n\")\n",
    "\n",
    "cv_results = model.spatial_cross_validation(\n",
    "    X=X,\n",
    "    y=y,\n",
    "    locations=all_locations_gdf,\n",
    "    n_folds=5\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Cross-Validation Results:\")\n",
    "print(\"=\"*60)\n",
    "for metric, value in cv_results.items():\n",
    "    if 'mean' in metric:\n",
    "        metric_name = metric.replace('_mean', '')\n",
    "        std = cv_results.get(f'{metric_name}_std', 0)\n",
    "        print(f\"{metric_name.upper():12s}: {value:.3f} (+/- {std:.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Train Final Model\n",
    "\n",
    "Train on all data for deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train final model\n",
    "print(\"Training final model on all data...\\n\")\n",
    "\n",
    "model.train(X, y)\n",
    "\n",
    "print(\"\\n✓ Model training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Feature Importance Analysis\n",
    "\n",
    "Understand which features matter most for prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot feature importance\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "top_features = model.feature_importance.head(10)\n",
    "\n",
    "sns.barplot(\n",
    "    data=top_features,\n",
    "    y='feature',\n",
    "    x='importance',\n",
    "    ax=ax\n",
    ")\n",
    "\n",
    "ax.set_title('Top 10 Most Important Features', fontsize=16, fontweight='bold')\n",
    "ax.set_xlabel('Importance Score')\n",
    "ax.set_ylabel('Feature')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nTop 10 Most Important Features:\")\n",
    "print(top_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Evaluate Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on training data (for visualization)\n",
    "y_pred = model.predict(X)\n",
    "y_pred_proba = model.predict_proba(X)[:, 1]\n",
    "\n",
    "# Calculate metrics\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "print(\"Model Performance on Training Data:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Accuracy:  {accuracy_score(y, y_pred):.3f}\")\n",
    "print(f\"Precision: {precision_score(y, y_pred):.3f}\")\n",
    "print(f\"Recall:    {recall_score(y, y_pred):.3f}\")\n",
    "print(f\"F1 Score:  {f1_score(y, y_pred):.3f}\")\n",
    "print(f\"AUC:       {roc_auc_score(y, y_pred_proba):.3f}\")\n",
    "\n",
    "print(\"\\n⚠️ Note: These are training metrics. CV metrics above are more reliable!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot prediction distribution\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Probability distribution by class\n",
    "ax1.hist(y_pred_proba[y == 1], bins=30, alpha=0.7, label='Positive (Coffee Shops)', color='red')\n",
    "ax1.hist(y_pred_proba[y == 0], bins=30, alpha=0.7, label='Negative (Random)', color='blue')\n",
    "ax1.set_xlabel('Predicted Probability')\n",
    "ax1.set_ylabel('Count')\n",
    "ax1.set_title('Prediction Distribution by Class')\n",
    "ax1.legend()\n",
    "\n",
    "# Score distribution\n",
    "scores = y_pred_proba * 100\n",
    "ax2.hist(scores, bins=30, alpha=0.7, color='green')\n",
    "ax2.set_xlabel('Suitability Score (0-100)')\n",
    "ax2.set_ylabel('Count')\n",
    "ax2.set_title('Site Suitability Score Distribution')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Save Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "model_path = Path('../models/coffee_shop_rf_model.pkl')\n",
    "model.save_model(str(model_path))\n",
    "\n",
    "print(f\"✓ Model saved to: {model_path}\")\n",
    "print(f\"\\nModel size: {model_path.stat().st_size / 1024:.1f} KB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Demo - Score New Locations\n",
    "\n",
    "Test the model on new candidate locations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate some test locations\n",
    "test_locations = generate_negative_samples(\n",
    "    positive_samples=positive_samples,\n",
    "    bbox=JAKARTA_BBOX,\n",
    "    n_samples=10,\n",
    "    min_distance=0  # Allow any distance for testing\n",
    ")\n",
    "\n",
    "print(f\"Generated {len(test_locations)} test locations\")\n",
    "\n",
    "# Calculate features for test locations\n",
    "test_features = engineer.calculate_all_buffer_features(\n",
    "    locations=test_locations,\n",
    "    competitors=positive_samples,\n",
    "    all_pois=None\n",
    ")\n",
    "\n",
    "# Score locations\n",
    "test_scores = model.score_location(test_features.values)\n",
    "\n",
    "# Add coordinates\n",
    "test_scores['latitude'] = [geom.y for geom in test_locations.geometry]\n",
    "test_scores['longitude'] = [geom.x for geom in test_locations.geometry]\n",
    "\n",
    "# Sort by score\n",
    "test_scores_sorted = test_scores.sort_values('score', ascending=False)\n",
    "\n",
    "print(\"\\nTest Location Scores:\")\n",
    "print(test_scores_sorted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Model Training Complete! ✓\n",
    "\n",
    "**Model Performance:**\n",
    "- Cross-validation accuracy: Check results above (target: 70-80%)\n",
    "- Uses spatial CV to prevent overfitting\n",
    "- Random Forest with 200 trees\n",
    "\n",
    "**Key Features:**\n",
    "- Competitor density (most important)\n",
    "- Distance to nearest competitor\n",
    "- POI diversity indices\n",
    "\n",
    "**Next Steps:**\n",
    "1. ✅ Model trained and saved\n",
    "2. ➡️ Use `05_prediction_demo.ipynb` to score new locations\n",
    "3. ➡️ Validate predictions against real coffee shop openings\n",
    "4. ➡️ Integrate into web application for customer use\n",
    "\n",
    "**Model File:** `models/coffee_shop_rf_model.pkl`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
