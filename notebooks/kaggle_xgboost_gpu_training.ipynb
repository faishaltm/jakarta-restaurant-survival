{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Restaurant Survival - XGBoost GPU Training\n",
    "\n",
    "**üöÄ GPU-Accelerated**: 10-50x faster than CPU-only methods\n",
    "\n",
    "**Requirements**:\n",
    "- Enable **GPU T4 x2** in Kaggle\n",
    "- Upload `jakarta_restaurant_features_complete.csv`\n",
    "\n",
    "**Expected Time**: 5-10 minutes (vs hours with scikit-survival)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install\n",
    "!pip install -q xgboost scikit-survival"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import time\n",
    "import gc\n",
    "\n",
    "import xgboost as xgb\n",
    "from xgboost import DMatrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sksurv.metrics import concordance_index_censored\n",
    "\n",
    "print(\"‚úÖ Imports complete\")\n",
    "print(f\"   XGBoost version: {xgb.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config\n",
    "DATA_PATH = Path('/kaggle/input') if Path('/kaggle/input').exists() else Path('data')\n",
    "OUTPUT_PATH = Path('/kaggle/working') if Path('/kaggle').exists() else Path('outputs')\n",
    "OUTPUT_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "XGBOOST_PARAMS = {\n",
    "    'objective': 'survival:cox',\n",
    "    'eval_metric': 'cox-nloglik',\n",
    "    'tree_method': 'hist',\n",
    "    'device': 'cuda',            # GPU!\n",
    "    'max_depth': 6,\n",
    "    'learning_rate': 0.1,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'min_child_weight': 5,\n",
    "    'reg_alpha': 0.1,\n",
    "    'reg_lambda': 1.0,\n",
    "    'random_state': 42\n",
    "}\n",
    "\n",
    "print(f\"üìÅ Data: {DATA_PATH}\")\n",
    "print(f\"üìÅ Output: {OUTPUT_PATH}\")\n",
    "print(f\"üéØ Device: {XGBOOST_PARAMS['device']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "df = pd.read_csv(DATA_PATH / 'jakarta_restaurant_features_complete.csv')\n",
    "print(f\"‚úÖ Loaded {len(df):,} restaurants\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter mature only\n",
    "df_mature = df[df['categorical_label'] != 2].copy()\n",
    "\n",
    "print(f\"‚úÖ Mature: {len(df_mature):,}\")\n",
    "print(f\"   Failed: {(df_mature['event_observed'] == 1).sum():,} ({(df_mature['event_observed'] == 1).mean():.1%})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get features\n",
    "exclude = ['osm_id', 'name', 'poi_type', 'date_created', 'date_closed', \n",
    "           'survival_days', 'event_observed', 'categorical_label', 'geometry', 'lat', 'lon']\n",
    "feature_cols = [c for c in df_mature.columns if c not in exclude]\n",
    "\n",
    "# Fill missing\n",
    "df_mature[feature_cols] = df_mature[feature_cols].fillna(df_mature[feature_cols].median())\n",
    "\n",
    "print(f\"‚úÖ Features: {len(feature_cols)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Prepare XGBoost survival data\n# For XGBoost survival:cox, we need to create labels as: -survival_days for events, +survival_days for censored\n# Negative = event occurred, Positive = censored\n\ny_train_xgb = df_mature['survival_days'].copy().astype(float)\n# Make negative for events (deaths/failures)\ny_train_xgb[df_mature['event_observed'] == 1] *= -1\n\nX = df_mature[feature_cols].values\nevents = df_mature['event_observed'].values\n\nprint(f\"‚úÖ Data prepared for XGBoost survival:cox\")\nprint(f\"   X shape: {X.shape}\")\nprint(f\"   Label range: {y_train_xgb.min():.0f} to {y_train_xgb.max():.0f}\")\nprint(f\"   Events (negative): {(y_train_xgb < 0).sum():,}\")\nprint(f\"   Censored (positive): {(y_train_xgb > 0).sum():,}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Split\nX_train, X_test, y_train, y_test, event_train, event_test = train_test_split(\n    X, y_train_xgb, events, test_size=0.2, random_state=42, stratify=events\n)\n\nprint(f\"‚úÖ Split: Train {len(X_train):,} | Test {len(X_test):,}\")\nprint(f\"   Train events: {(y_train < 0).sum():,}\")\nprint(f\"   Test events: {(y_test < 0).sum():,}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Create DMatrix (simple format for survival:cox)\n# For survival:cox, label is signed: negative = event, positive = censored\ndtrain = DMatrix(X_train, label=y_train, feature_names=feature_cols)\ndtest = DMatrix(X_test, label=y_test, feature_names=feature_cols)\n\nprint(f\"‚úÖ DMatrix created (GPU-ready)\")\nprint(f\"   Train: {dtrain.num_row():,} x {dtrain.num_col()}\")\nprint(f\"   Test: {dtest.num_row():,} x {dtest.num_col()}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training with ALL Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üî• Training XGBoost (500 trees) on GPU...\")\n",
    "start = time.time()\n",
    "\n",
    "model = xgb.train(\n",
    "    XGBOOST_PARAMS,\n",
    "    dtrain,\n",
    "    num_boost_round=500,\n",
    "    evals=[(dtrain, 'train'), (dtest, 'test')],\n",
    "    early_stopping_rounds=50,\n",
    "    verbose_eval=50\n",
    ")\n",
    "\n",
    "elapsed = time.time() - start\n",
    "print(f\"\\n‚úÖ Training done in {elapsed:.1f}s\")\n",
    "print(f\"   Speed: {500/elapsed:.1f} trees/second\")\n",
    "print(f\"   Best iteration: {model.best_iteration}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Predict\npred_train = model.predict(dtrain)\npred_test = model.predict(dtest)\n\n# For C-index calculation, we need absolute values of labels\ny_train_abs = np.abs(y_train)\ny_test_abs = np.abs(y_test)\n\n# C-index\nc_train = concordance_index_censored(event_train.astype(bool), y_train_abs, pred_train)[0]\nc_test = concordance_index_censored(event_test.astype(bool), y_test_abs, pred_test)[0]\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"üéØ RESULTS (ALL FEATURES)\")\nprint(\"=\"*60)\nprint(f\"\\n   Train C-index: {c_train:.4f}\")\nprint(f\"   Test C-index:  {c_test:.4f}\")\nprint(f\"   Overfitting:   {c_train - c_test:.4f}\")\nprint(f\"\\n   Features: {len(feature_cols)}\")\nprint(f\"   Trees: {model.best_iteration}\")\nprint(\"\\n\" + \"=\"*60)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get importance\n",
    "importance_dict = model.get_score(importance_type='gain')\n",
    "importance_df = pd.DataFrame({\n",
    "    'feature': list(importance_dict.keys()),\n",
    "    'importance': list(importance_dict.values())\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "importance_df['importance_pct'] = importance_df['importance'] / importance_df['importance'].sum() * 100\n",
    "\n",
    "print(\"\\nüìä Top 20 Features:\")\n",
    "print(importance_df.head(20)[['feature', 'importance_pct']].to_string(index=False))\n",
    "\n",
    "# Save\n",
    "importance_df.to_csv(OUTPUT_PATH / 'feature_importance.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot\n",
    "plt.figure(figsize=(10, 8))\n",
    "top20 = importance_df.head(20)\n",
    "plt.barh(range(len(top20)), top20['importance_pct'])\n",
    "plt.yticks(range(len(top20)), top20['feature'], fontsize=9)\n",
    "plt.xlabel('Importance (%)')\n",
    "plt.title('Top 20 Features - XGBoost Survival')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_PATH / 'importance.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Top-K Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"üî• Testing different K values...\")\n\nk_values = [10, 20, 30, 40, 50]\nresults = []\n\nfor k in k_values:\n    print(f\"\\n  K={k}...\", end=\" \")\n    start = time.time()\n    \n    # Select top k\n    top_k = importance_df.head(k)['feature'].tolist()\n    k_indices = [feature_cols.index(f) for f in top_k]\n    \n    # Create DMatrix\n    dtrain_k = DMatrix(X_train[:, k_indices], label=y_train)\n    dtest_k = DMatrix(X_test[:, k_indices], label=y_test)\n    \n    # Train\n    model_k = xgb.train(\n        XGBOOST_PARAMS,\n        dtrain_k,\n        num_boost_round=300,\n        early_stopping_rounds=30,\n        evals=[(dtest_k, 'test')],\n        verbose_eval=False\n    )\n    \n    # Evaluate\n    pred_k = model_k.predict(dtest_k)\n    y_test_abs = np.abs(y_test)\n    c_k = concordance_index_censored(event_test.astype(bool), y_test_abs, pred_k)[0]\n    \n    elapsed = time.time() - start\n    results.append({'k': k, 'c_index': c_k, 'time_s': elapsed})\n    \n    print(f\"C-index: {c_k:.4f} ({elapsed:.1f}s)\")\n    \n    del model_k, dtrain_k, dtest_k\n    gc.collect()\n\ndf_results = pd.DataFrame(results)\nprint(\"\\nüìä Results:\")\nprint(df_results.to_string(index=False))\n\ndf_results.to_csv(OUTPUT_PATH / 'top_k_results.csv', index=False)\n\nbest = df_results.loc[df_results['c_index'].idxmax()]\nprint(f\"\\nüèÜ Best: k={int(best['k'])} ‚Üí C-index={best['c_index']:.4f}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üìù FINAL SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\n‚úÖ Dataset:\")\n",
    "print(f\"   Restaurants: {len(df_mature):,}\")\n",
    "print(f\"   Failures: {events.sum():,} ({events.mean():.1%})\")\n",
    "print(f\"   Features: {len(feature_cols)}\")\n",
    "\n",
    "print(f\"\\n‚úÖ Performance:\")\n",
    "print(f\"   All features C-index: {c_test:.4f}\")\n",
    "print(f\"   Best K={int(best['k'])} C-index: {best['c_index']:.4f}\")\n",
    "\n",
    "print(f\"\\n‚úÖ Top 5 Features:\")\n",
    "for i, row in importance_df.head(5).iterrows():\n",
    "    print(f\"   {i+1}. {row['feature']:40s} ({row['importance_pct']:.2f}%)\")\n",
    "\n",
    "print(f\"\\n‚úÖ Training Speed:\")\n",
    "print(f\"   GPU-accelerated: {elapsed:.1f}s for 500 trees\")\n",
    "print(f\"   Speed: {500/elapsed:.1f} trees/second\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üéâ TRAINING COMPLETE!\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}