{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Restaurant Survival - XGBoost Progressive Feature Training\n",
    "\n",
    "**üöÄ GPU-Accelerated Progressive Training**\n",
    "\n",
    "**Strategy**: Start from proven features, add progressively\n",
    "\n",
    "**Dataset**: 72,082 restaurants (5.5% failure - imbalanced!)\n",
    "\n",
    "**Phases**:\n",
    "1. Baseline: 8 proven features (Target C-index >0.70)\n",
    "2. +Indonesia features (Target +5-10%)\n",
    "3. +Interaction features (Target +2-5%)\n",
    "4. +Entropy features (Target +5%)\n",
    "5. Test 5 imbalance strategies\n",
    "\n",
    "**Expected Time**: 20-30 minutes total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q xgboost scikit-survival"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import time\n",
    "import gc\n",
    "import json\n",
    "\n",
    "import xgboost as xgb\n",
    "from xgboost import DMatrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "from sksurv.metrics import concordance_index_censored\n",
    "\n",
    "print(\"‚úÖ Imports complete\")\n",
    "print(f\"   XGBoost: {xgb.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "DATA_PATH = Path('/kaggle/input/jakarta-restaurant-features-complete')\n",
    "OUTPUT_PATH = Path('/kaggle/working')\n",
    "OUTPUT_PATH.mkdir(exist_ok=True)\n",
    "\n",
    "# XGBoost base config (will modify for imbalance strategies)\n",
    "XGBOOST_BASE = {\n",
    "    'objective': 'survival:cox',\n",
    "    'eval_metric': 'cox-nloglik',\n",
    "    'tree_method': 'hist',\n",
    "    'device': 'cuda',\n",
    "    'max_depth': 6,\n",
    "    'learning_rate': 0.1,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'min_child_weight': 5,\n",
    "    'reg_alpha': 0.1,\n",
    "    'reg_lambda': 1.0,\n",
    "    'random_state': 42\n",
    "}\n",
    "\n",
    "print(f\"üìÅ Data: {DATA_PATH}\")\n",
    "print(f\"üìÅ Output: {OUTPUT_PATH}\")\n",
    "print(f\"üéØ Device: {XGBOOST_BASE['device']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "df = pd.read_csv(DATA_PATH / 'jakarta_restaurant_features_complete.csv')\n",
    "df_mature = df[df['categorical_label'] != 2].copy()\n",
    "\n",
    "print(f\"‚úÖ Loaded: {len(df_mature):,} mature restaurants\")\n",
    "print(f\"   Failures: {(df_mature['event_observed'] == 1).sum():,} ({(df_mature['event_observed'] == 1).mean():.1%})\")\n",
    "print(f\"   Successes: {(df_mature['event_observed'] == 0).sum():,} ({(df_mature['event_observed'] == 0).mean():.1%})\")\n",
    "print(f\"\\n‚ö†Ô∏è  Imbalance ratio: {(df_mature['event_observed'] == 0).sum() / (df_mature['event_observed'] == 1).sum():.1f}:1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define feature groups for progressive training\n",
    "FEATURE_PHASES = {\n",
    "    'Phase 1 - Proven Core (8)': [\n",
    "        'nearest_gas_station_m',      # 80% importance (SPBU)\n",
    "        'transport_density_1km',       # 67% importance\n",
    "        'transport_count_1000m',       # 56% importance  \n",
    "        'working_age_district',        # 48% importance\n",
    "        'density_district',            # 45% importance\n",
    "        'competitors_count_5000m',     # Competition\n",
    "        'dist_city_center_km',         # Accessibility\n",
    "        'pasar_count_5000m'            # Indonesia-specific (5km buffer)\n",
    "    ],\n",
    "    \n",
    "    'Phase 2 - Indonesia Features (10)': [\n",
    "        'mosque_count_500m',\n",
    "        'mosque_count_1000m', \n",
    "        'nearest_mosque_m',\n",
    "        'pasar_count_1000m',\n",
    "        'nearest_pasar_m',\n",
    "        'convenience_count_1000m',\n",
    "        'gas_station_count_2000m',\n",
    "        'friday_prayer_impact',\n",
    "        'pasar_proximity_score',\n",
    "        'gas_proximity_score'\n",
    "    ],\n",
    "    \n",
    "    'Phase 3 - Interactions (8)': [\n",
    "        'income_pop_interaction',\n",
    "        'working_age_mall_inv',\n",
    "        'office_transport',\n",
    "        'demand_supply_ratio',\n",
    "        'mosque_residential',\n",
    "        'pasar_transport',\n",
    "        'cannibalization_risk_500m',\n",
    "        'urban_centrality'\n",
    "    ],\n",
    "    \n",
    "    'Phase 4 - Entropy (3)': [\n",
    "        'entropy_500m',\n",
    "        'entropy_1000m',\n",
    "        'entropy_2000m'\n",
    "    ]\n",
    "}\n",
    "\n",
    "print(\"üìã Feature Phases Defined:\")\n",
    "total = 0\n",
    "for phase, features in FEATURE_PHASES.items():\n",
    "    total += len(features)\n",
    "    print(f\"   {phase}: {len(features)} features (cumulative: {total})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare base data\n",
    "exclude = ['osm_id', 'name', 'poi_type', 'date_created', 'date_closed',\n",
    "           'survival_days', 'event_observed', 'categorical_label', 'geometry', 'lat', 'lon']\n",
    "\n",
    "all_features = [c for c in df_mature.columns if c not in exclude]\n",
    "\n",
    "# Fill missing\n",
    "df_mature[all_features] = df_mature[all_features].fillna(df_mature[all_features].median())\n",
    "\n",
    "# Prepare XGBoost survival labels (negative = event, positive = censored)\n",
    "y_xgb = df_mature['survival_days'].copy().astype(float)\n",
    "y_xgb[df_mature['event_observed'] == 1] *= -1\n",
    "\n",
    "events = df_mature['event_observed'].values\n",
    "y_abs = np.abs(y_xgb)  # For C-index calculation\n",
    "\n",
    "print(f\"‚úÖ Data prepared\")\n",
    "print(f\"   Total features available: {len(all_features)}\")\n",
    "print(f\"   Label range: {y_xgb.min():.0f} to {y_xgb.max():.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(X_train, X_test, y_train, y_test, event_test, y_test_abs, \n",
    "                       feature_names, params, n_rounds=300, phase_name=\"\"):\n",
    "    \"\"\"Train XGBoost and return results\"\"\"\n",
    "    \n",
    "    start = time.time()\n",
    "    \n",
    "    # Create DMatrix\n",
    "    dtrain = DMatrix(X_train, label=y_train, feature_names=feature_names)\n",
    "    dtest = DMatrix(X_test, label=y_test, feature_names=feature_names)\n",
    "    \n",
    "    # Train\n",
    "    model = xgb.train(\n",
    "        params,\n",
    "        dtrain,\n",
    "        num_boost_round=n_rounds,\n",
    "        evals=[(dtest, 'test')],\n",
    "        early_stopping_rounds=30,\n",
    "        verbose_eval=False\n",
    "    )\n",
    "    \n",
    "    # Predict\n",
    "    pred = model.predict(dtest)\n",
    "    \n",
    "    # C-index\n",
    "    c_index = concordance_index_censored(event_test.astype(bool), y_test_abs, pred)[0]\n",
    "    \n",
    "    elapsed = time.time() - start\n",
    "    \n",
    "    # Feature importance\n",
    "    importance = model.get_score(importance_type='gain')\n",
    "    \n",
    "    result = {\n",
    "        'phase': phase_name,\n",
    "        'n_features': len(feature_names),\n",
    "        'c_index': c_index,\n",
    "        'best_iteration': model.best_iteration,\n",
    "        'time_s': elapsed,\n",
    "        'model': model,\n",
    "        'importance': importance\n",
    "    }\n",
    "    \n",
    "    print(f\"   C-index: {c_index:.4f} | Trees: {model.best_iteration} | Time: {elapsed:.1f}s\")\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Progressive Training - Standard (No Imbalance Handling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üî• PROGRESSIVE FEATURE TRAINING (Standard - No Weighting)\\n\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "progressive_results = []\n",
    "cumulative_features = []\n",
    "\n",
    "for phase_name, phase_features in FEATURE_PHASES.items():\n",
    "    \n",
    "    # Add features cumulatively\n",
    "    cumulative_features.extend(phase_features)\n",
    "    \n",
    "    # Check if features exist\n",
    "    valid_features = [f for f in cumulative_features if f in all_features]\n",
    "    missing = [f for f in cumulative_features if f not in all_features]\n",
    "    \n",
    "    if missing:\n",
    "        print(f\"\\n‚ö†Ô∏è  {phase_name}: {len(missing)} features not found:\")\n",
    "        for f in missing[:5]:\n",
    "            print(f\"     - {f}\")\n",
    "        if len(missing) > 5:\n",
    "            print(f\"     ... and {len(missing)-5} more\")\n",
    "    \n",
    "    if not valid_features:\n",
    "        print(f\"\\n‚ùå {phase_name}: No valid features! Skipping...\")\n",
    "        continue\n",
    "    \n",
    "    print(f\"\\n{phase_name}\")\n",
    "    print(f\"  Features: {len(valid_features)} (added {len(phase_features)})\")\n",
    "    \n",
    "    # Get feature indices\n",
    "    indices = [all_features.index(f) for f in valid_features]\n",
    "    X = df_mature[valid_features].values\n",
    "    \n",
    "    # Split\n",
    "    X_train, X_test, y_train, y_test, event_train, event_test = train_test_split(\n",
    "        X, y_xgb, events, test_size=0.2, random_state=42, stratify=events\n",
    "    )\n",
    "    \n",
    "    y_test_abs = np.abs(y_test)\n",
    "    \n",
    "    # Train\n",
    "    result = train_and_evaluate(\n",
    "        X_train, X_test, y_train, y_test, event_test, y_test_abs,\n",
    "        valid_features, XGBOOST_BASE, n_rounds=300, phase_name=phase_name\n",
    "    )\n",
    "    \n",
    "    progressive_results.append(result)\n",
    "    \n",
    "    # Cleanup\n",
    "    gc.collect()\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"‚úÖ Progressive training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary table\n",
    "df_progressive = pd.DataFrame([{\n",
    "    'phase': r['phase'],\n",
    "    'n_features': r['n_features'],\n",
    "    'c_index': r['c_index'],\n",
    "    'trees': r['best_iteration'],\n",
    "    'time_s': r['time_s']\n",
    "} for r in progressive_results])\n",
    "\n",
    "print(\"\\nüìä Progressive Results Summary:\")\n",
    "print(\"=\"*70)\n",
    "print(df_progressive.to_string(index=False))\n",
    "\n",
    "# Save\n",
    "df_progressive.to_csv(OUTPUT_PATH / 'progressive_results_standard.csv', index=False)\n",
    "\n",
    "# Best phase\n",
    "best_idx = df_progressive['c_index'].idxmax()\n",
    "best = df_progressive.iloc[best_idx]\n",
    "\n",
    "print(f\"\\nüèÜ Best Phase: {best['phase']}\")\n",
    "print(f\"   Features: {int(best['n_features'])}\")\n",
    "print(f\"   C-index: {best['c_index']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 5: Test Imbalance Strategies\n",
    "\n",
    "Test 5 strategies with the BEST feature set from progressive training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use best feature set\n",
    "best_features_idx = best_idx\n",
    "best_features = [f for f in cumulative_features if f in all_features]\n",
    "\n",
    "print(f\"üî• TESTING 5 IMBALANCE STRATEGIES\")\n",
    "print(f\"   Using: {len(best_features)} features from {progressive_results[best_features_idx]['phase']}\")\n",
    "print(f\"\\n\" + \"=\"*70)\n",
    "\n",
    "# Prepare data\n",
    "X = df_mature[best_features].values\n",
    "X_train, X_test, y_train, y_test, event_train, event_test = train_test_split(\n",
    "    X, y_xgb, events, test_size=0.2, random_state=42, stratify=events\n",
    ")\n",
    "y_test_abs = np.abs(y_test)\n",
    "\n",
    "# Calculate imbalance ratio\n",
    "failure_rate = event_train.mean()\n",
    "success_rate = 1 - failure_rate\n",
    "scale_pos_weight = success_rate / failure_rate\n",
    "\n",
    "print(f\"   Imbalance: {success_rate:.1%} success / {failure_rate:.1%} failure\")\n",
    "print(f\"   Scale weight: {scale_pos_weight:.2f}\\n\")\n",
    "\n",
    "imbalance_results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strategy 1: Standard (No weighting)\n",
    "print(\"1Ô∏è‚É£  Standard (No Imbalance Handling)\")\n",
    "\n",
    "result = train_and_evaluate(\n",
    "    X_train, X_test, y_train, y_test, event_test, y_test_abs,\n",
    "    best_features, XGBOOST_BASE, phase_name=\"Standard\"\n",
    ")\n",
    "imbalance_results.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strategy 2: Weighted (scale_pos_weight)\n",
    "print(\"\\n2Ô∏è‚É£  Weighted (scale_pos_weight)\")\n",
    "\n",
    "params_weighted = XGBOOST_BASE.copy()\n",
    "params_weighted['scale_pos_weight'] = scale_pos_weight\n",
    "\n",
    "result = train_and_evaluate(\n",
    "    X_train, X_test, y_train, y_test, event_test, y_test_abs,\n",
    "    best_features, params_weighted, phase_name=\"Weighted\"\n",
    ")\n",
    "imbalance_results.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strategy 3: Sample Weights (manual)\n",
    "print(\"\\n3Ô∏è‚É£  Sample Weights (Manual)\")\n",
    "\n",
    "# Compute sample weights\n",
    "sample_weights = compute_sample_weight('balanced', event_train)\n",
    "\n",
    "start = time.time()\n",
    "dtrain_w = DMatrix(X_train, label=y_train, weight=sample_weights, feature_names=best_features)\n",
    "dtest_w = DMatrix(X_test, label=y_test, feature_names=best_features)\n",
    "\n",
    "model_w = xgb.train(\n",
    "    XGBOOST_BASE,\n",
    "    dtrain_w,\n",
    "    num_boost_round=300,\n",
    "    evals=[(dtest_w, 'test')],\n",
    "    early_stopping_rounds=30,\n",
    "    verbose_eval=False\n",
    ")\n",
    "\n",
    "pred_w = model_w.predict(dtest_w)\n",
    "c_w = concordance_index_censored(event_test.astype(bool), y_test_abs, pred_w)[0]\n",
    "elapsed = time.time() - start\n",
    "\n",
    "result = {\n",
    "    'phase': 'Sample Weights',\n",
    "    'n_features': len(best_features),\n",
    "    'c_index': c_w,\n",
    "    'best_iteration': model_w.best_iteration,\n",
    "    'time_s': elapsed,\n",
    "    'model': model_w,\n",
    "    'importance': model_w.get_score(importance_type='gain')\n",
    "}\n",
    "\n",
    "print(f\"   C-index: {c_w:.4f} | Trees: {model_w.best_iteration} | Time: {elapsed:.1f}s\")\n",
    "imbalance_results.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Strategy 4: Undersampling\nprint(\"\\n4Ô∏è‚É£  Undersampling (Balance 1:3)\")\n\n# Convert to numpy arrays if pandas Series\ny_train_np = y_train.values if hasattr(y_train, 'values') else y_train\ny_test_np = y_test.values if hasattr(y_test, 'values') else y_test\n\n# Sample to balance\nfailures_idx = np.where(event_train == 1)[0]\nsuccesses_idx = np.where(event_train == 0)[0]\n\n# Keep all failures, undersample successes (1:3 ratio)\nn_failures = len(failures_idx)\nn_successes_sample = n_failures * 3\n\nnp.random.seed(42)\nsuccesses_sampled = np.random.choice(successes_idx, size=n_successes_sample, replace=False)\n\nbalanced_idx = np.concatenate([failures_idx, successes_sampled])\nnp.random.shuffle(balanced_idx)\n\nX_train_bal = X_train[balanced_idx]\ny_train_bal = y_train_np[balanced_idx]\n\nprint(f\"   Original: {len(X_train):,} | Balanced: {len(X_train_bal):,}\")\n\nresult = train_and_evaluate(\n    X_train_bal, X_test, y_train_bal, y_test_np, event_test, y_test_abs,\n    best_features, XGBOOST_BASE, phase_name=\"Undersampled\"\n)\nimbalance_results.append(result)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Strategy 5: Hybrid (Weighted + Undersampled)\nprint(\"\\n5Ô∏è‚É£  Hybrid (Weighted + Undersampled)\")\n\nparams_hybrid = XGBOOST_BASE.copy()\nparams_hybrid['scale_pos_weight'] = 3.0  # Moderate weight for already balanced data\n\nresult = train_and_evaluate(\n    X_train_bal, X_test, y_train_bal, y_test_np, event_test, y_test_abs,\n    best_features, params_hybrid, phase_name=\"Hybrid\"\n)\nimbalance_results.append(result)\n\nprint(\"\\n\" + \"=\"*70)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imbalance strategy comparison\n",
    "df_imbalance = pd.DataFrame([{\n",
    "    'strategy': r['phase'],\n",
    "    'c_index': r['c_index'],\n",
    "    'trees': r['best_iteration'],\n",
    "    'time_s': r['time_s']\n",
    "} for r in imbalance_results])\n",
    "\n",
    "print(\"\\nüìä Imbalance Strategy Comparison:\")\n",
    "print(\"=\"*70)\n",
    "print(df_imbalance.to_string(index=False))\n",
    "\n",
    "df_imbalance.to_csv(OUTPUT_PATH / 'imbalance_strategy_results.csv', index=False)\n",
    "\n",
    "best_strategy_idx = df_imbalance['c_index'].idxmax()\n",
    "best_strategy = df_imbalance.iloc[best_strategy_idx]\n",
    "\n",
    "print(f\"\\nüèÜ Best Strategy: {best_strategy['strategy']}\")\n",
    "print(f\"   C-index: {best_strategy['c_index']:.4f}\")\n",
    "print(f\"   Improvement: +{(best_strategy['c_index'] - df_imbalance.iloc[0]['c_index'])*100:.2f}% vs Standard\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get best model\n",
    "best_model = imbalance_results[best_strategy_idx]['model']\n",
    "importance_dict = best_model.get_score(importance_type='gain')\n",
    "\n",
    "importance_df = pd.DataFrame({\n",
    "    'feature': list(importance_dict.keys()),\n",
    "    'importance': list(importance_dict.values())\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "importance_df['importance_pct'] = importance_df['importance'] / importance_df['importance'].sum() * 100\n",
    "\n",
    "print(\"\\nüìä Top 20 Feature Importance (Best Model):\")\n",
    "print(\"=\"*70)\n",
    "print(importance_df.head(20)[['feature', 'importance_pct']].to_string(index=False))\n",
    "\n",
    "importance_df.to_csv(OUTPUT_PATH / 'final_feature_importance.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# 1. Progressive C-index\n",
    "ax1 = axes[0, 0]\n",
    "ax1.plot(range(len(df_progressive)), df_progressive['c_index'], marker='o', linewidth=2, markersize=8)\n",
    "ax1.set_xticks(range(len(df_progressive)))\n",
    "ax1.set_xticklabels([p.replace('Phase ', 'P') for p in df_progressive['phase']], rotation=45, ha='right')\n",
    "ax1.set_ylabel('C-index')\n",
    "ax1.set_title('Progressive Feature Addition Performance')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.axhline(y=0.7, color='g', linestyle='--', alpha=0.5, label='Target 0.70')\n",
    "ax1.legend()\n",
    "\n",
    "# 2. Imbalance strategies\n",
    "ax2 = axes[0, 1]\n",
    "ax2.barh(range(len(df_imbalance)), df_imbalance['c_index'])\n",
    "ax2.set_yticks(range(len(df_imbalance)))\n",
    "ax2.set_yticklabels(df_imbalance['strategy'])\n",
    "ax2.set_xlabel('C-index')\n",
    "ax2.set_title('Imbalance Strategy Comparison')\n",
    "ax2.invert_yaxis()\n",
    "\n",
    "# 3. Top 15 features\n",
    "ax3 = axes[1, 0]\n",
    "top15 = importance_df.head(15)\n",
    "ax3.barh(range(len(top15)), top15['importance_pct'])\n",
    "ax3.set_yticks(range(len(top15)))\n",
    "ax3.set_yticklabels(top15['feature'], fontsize=9)\n",
    "ax3.set_xlabel('Importance (%)')\n",
    "ax3.set_title('Top 15 Features (Best Model)')\n",
    "ax3.invert_yaxis()\n",
    "\n",
    "# 4. Features vs C-index\n",
    "ax4 = axes[1, 1]\n",
    "ax4.scatter(df_progressive['n_features'], df_progressive['c_index'], s=100, alpha=0.6)\n",
    "ax4.set_xlabel('Number of Features')\n",
    "ax4.set_ylabel('C-index')\n",
    "ax4.set_title('Features vs Performance')\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_PATH / 'training_summary.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úÖ Visualizations saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Summary & Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üìù FINAL SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\n‚úÖ Dataset:\")\n",
    "print(f\"   Total: {len(df_mature):,} mature restaurants\")\n",
    "print(f\"   Failures: {(df_mature['event_observed'] == 1).sum():,} (5.5%)\")\n",
    "print(f\"   Imbalance: {scale_pos_weight:.1f}:1\")\n",
    "\n",
    "print(f\"\\n‚úÖ Best Configuration:\")\n",
    "print(f\"   Features: {int(best['n_features'])} ({best['phase']})\")\n",
    "print(f\"   Strategy: {best_strategy['strategy']}\")\n",
    "print(f\"   C-index: {best_strategy['c_index']:.4f}\")\n",
    "\n",
    "print(f\"\\n‚úÖ Top 5 Most Important Features:\")\n",
    "for i, row in importance_df.head(5).iterrows():\n",
    "    print(f\"   {i+1}. {row['feature']:40s} ({row['importance_pct']:.2f}%)\")\n",
    "\n",
    "print(f\"\\n‚úÖ Performance Progression:\")\n",
    "for i, row in df_progressive.iterrows():\n",
    "    print(f\"   {row['phase']:40s}: {row['c_index']:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "\n",
    "if best_strategy['c_index'] >= 0.70:\n",
    "    print(\"‚úÖ SUCCESS: C-index ‚â• 0.70 (Good discriminative power!)\")\n",
    "elif best_strategy['c_index'] >= 0.60:\n",
    "    print(\"‚ö†Ô∏è  MODERATE: C-index 0.60-0.70 (Acceptable but needs improvement)\")\n",
    "else:\n",
    "    print(\"‚ùå POOR: C-index < 0.60 (Model needs significant improvement)\")\n",
    "\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final config\n",
    "final_config = {\n",
    "    'best_phase': best['phase'],\n",
    "    'best_features': best_features,\n",
    "    'n_features': int(best['n_features']),\n",
    "    'best_strategy': best_strategy['strategy'],\n",
    "    'c_index': float(best_strategy['c_index']),\n",
    "    'model_params': params_hybrid if best_strategy['strategy'] == 'Hybrid' else XGBOOST_BASE,\n",
    "    'imbalance_ratio': float(scale_pos_weight),\n",
    "    'top_5_features': importance_df.head(5)['feature'].tolist()\n",
    "}\n",
    "\n",
    "with open(OUTPUT_PATH / 'final_model_config.json', 'w') as f:\n",
    "    json.dump(final_config, f, indent=2)\n",
    "\n",
    "print(\"\\n‚úÖ Final configuration saved to final_model_config.json\")\n",
    "print(\"\\nüéâ TRAINING COMPLETE!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}