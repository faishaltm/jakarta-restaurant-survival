{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Restaurant Survival Prediction - GPU Accelerated with XGBoost\n\n**üöÄ GPU-Accelerated Training**: Uses XGBoost Survival with CUDA support\n\n**Prerequisites**: \n- Run `kaggle_feature_extraction_complete.ipynb` first\n- Enable **GPU T4 x2** in Kaggle notebook settings\n\n**Target**: C-index 0.85-0.90 in **<10 minutes** (vs hours with CPU)\n\n**Strategy**:\n1. Load pre-extracted features (130 features)\n2. Quick feature importance with XGBoost (GPU-accelerated)\n3. Test Top-K feature selections\n4. Final optimized model\n\n**Advantages over scikit-survival**:\n- ‚úÖ 10-50x faster with GPU\n- ‚úÖ Handles 130 features easily\n- ‚úÖ Better memory efficiency\n- ‚úÖ Native Cox regression support\n\n---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Install required packages\n!pip install -q xgboost scikit-survival"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Imports\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom pathlib import Path\nimport gc\nimport time\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# XGBoost for GPU acceleration\nimport xgboost as xgb\nfrom xgboost import DMatrix\n\n# Survival analysis metrics\nfrom sksurv.metrics import concordance_index_censored\nfrom sksurv.util import Surv\n\n# Model selection\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\nprint(\"‚úÖ Imports complete\")\n\n# Check GPU availability\nprint(\"\\nüîç GPU Check:\")\ntry:\n    print(f\"   XGBoost version: {xgb.__version__}\")\n    print(f\"   GPU available: {xgb.get_config()['use_rmm']}\")\nexcept:\n    print(\"   XGBoost installed (GPU support will be auto-detected)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Paths\nDATA_PATH = Path('/kaggle/input') if Path('/kaggle/input').exists() else Path('data')\nOUTPUT_PATH = Path('/kaggle/working') if Path('/kaggle').exists() else Path('outputs/survival_training_advanced')\nOUTPUT_PATH.mkdir(parents=True, exist_ok=True)\n\n# XGBoost Survival Configuration (GPU-accelerated)\nXGBOOST_CONFIG = {\n    'objective': 'survival:cox',  # Cox proportional hazards\n    'eval_metric': 'cox-nloglik',\n    'tree_method': 'hist',        # Will auto-detect GPU\n    'device': 'cuda',              # Force GPU usage\n    'max_depth': 6,\n    'learning_rate': 0.1,\n    'n_estimators': 500,\n    'subsample': 0.8,\n    'colsample_bytree': 0.8,\n    'min_child_weight': 5,\n    'gamma': 0.1,\n    'reg_alpha': 0.1,              # L1 regularization\n    'reg_lambda': 1.0,             # L2 regularization\n    'random_state': 42,\n    'n_jobs': -1\n}\n\n# Quick feature importance config (fewer trees)\nQUICK_XGBOOST_CONFIG = XGBOOST_CONFIG.copy()\nQUICK_XGBOOST_CONFIG['n_estimators'] = 100  # Faster for importance\n\n# Train/test split\nTEST_SIZE = 0.2\nRANDOM_STATE = 42\n\nprint(f\"üìÅ Data: {DATA_PATH}\")\nprint(f\"üìÅ Output: {OUTPUT_PATH}\")\nprint(f\"\\nüéØ XGBoost Config:\")\nprint(f\"   Device: {XGBOOST_CONFIG['device']}\")\nprint(f\"   Tree method: {XGBOOST_CONFIG['tree_method']}\")\nprint(f\"   N-estimators: {XGBOOST_CONFIG['n_estimators']}\")\nprint(f\"   Learning rate: {XGBOOST_CONFIG['learning_rate']}\")\nprint(f\"   Max depth: {XGBOOST_CONFIG['max_depth']}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Pre-Extracted Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the comprehensive feature set\n",
    "features_file = DATA_PATH / 'jakarta_restaurant_features_complete.csv'\n",
    "\n",
    "if not features_file.exists():\n",
    "    raise FileNotFoundError(\n",
    "        f\"‚ùå Features file not found: {features_file}\\n\"\n",
    "        \"Please run 'kaggle_feature_extraction_complete.ipynb' first!\"\n",
    "    )\n",
    "\n",
    "df = pd.read_csv(features_file)\n",
    "\n",
    "print(f\"‚úÖ Loaded {len(df):,} restaurants\")\n",
    "print(f\"‚úÖ Total columns: {len(df.columns)}\")\n",
    "print(f\"\\nüìä Data overview:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify survival labels exist\n",
    "required_cols = ['survival_days', 'event_observed', 'categorical_label']\n",
    "missing = [c for c in required_cols if c not in df.columns]\n",
    "if missing:\n",
    "    raise ValueError(f\"‚ùå Missing required columns: {missing}\")\n",
    "\n",
    "# Filter mature restaurants only (already done in extraction, but verify)\n",
    "df_mature = df[df['categorical_label'] != 2].copy()\n",
    "\n",
    "print(f\"‚úÖ Mature restaurants: {len(df_mature):,}\")\n",
    "print(f\"   - Survived: {(df_mature['event_observed'] == 0).sum():,}\")\n",
    "print(f\"   - Failed: {(df_mature['event_observed'] == 1).sum():,}\")\n",
    "print(f\"   - Failure rate: {(df_mature['event_observed'] == 1).mean():.1%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify feature columns (exclude metadata and labels)\n",
    "exclude_cols = [\n",
    "    'osm_id', 'name', 'poi_type', 'date_created', 'date_closed',\n",
    "    'survival_days', 'event_observed', 'categorical_label',\n",
    "    'geometry', 'lat', 'lon'\n",
    "]\n",
    "\n",
    "feature_cols = [c for c in df_mature.columns if c not in exclude_cols]\n",
    "\n",
    "print(f\"‚úÖ Total features available: {len(feature_cols)}\")\n",
    "print(f\"\\nüìã Feature columns:\")\n",
    "for i, col in enumerate(feature_cols, 1):\n",
    "    print(f\"  {i:2d}. {col}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "missing_counts = df_mature[feature_cols].isnull().sum()\n",
    "missing_features = missing_counts[missing_counts > 0]\n",
    "\n",
    "if len(missing_features) > 0:\n",
    "    print(f\"‚ö†Ô∏è  Features with missing values:\")\n",
    "    for col, count in missing_features.items():\n",
    "        pct = count / len(df_mature) * 100\n",
    "        print(f\"  - {col}: {count:,} ({pct:.1f}%)\")\n",
    "    \n",
    "    # Fill missing values with median\n",
    "    df_mature[feature_cols] = df_mature[feature_cols].fillna(df_mature[feature_cols].median())\n",
    "    print(\"\\n‚úÖ Filled missing values with median\")\n",
    "else:\n",
    "    print(\"‚úÖ No missing values found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Prepare data for XGBoost\n# XGBoost survival needs lower and upper bounds for censored data\n# For our case:\n# - event=1 (died): lower_bound = upper_bound = survival_days\n# - event=0 (censored): lower_bound = survival_days, upper_bound = +inf\n\ny_lower = df_mature['survival_days'].values\ny_upper = df_mature['survival_days'].copy()\ny_upper[df_mature['event_observed'] == 0] = np.inf  # Censored\n\nX = df_mature[feature_cols].values\n\nprint(f\"‚úÖ Created XGBoost survival data\")\nprint(f\"   - X shape: {X.shape}\")\nprint(f\"   - Events: {(df_mature['event_observed'] == 1).sum():,}\")\nprint(f\"   - Censored: {(df_mature['event_observed'] == 0).sum():,}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Train/test split\nX_train, X_test, y_lower_train, y_lower_test, y_upper_train, y_upper_test, event_train, event_test = train_test_split(\n    X, y_lower, y_upper, df_mature['event_observed'].values,\n    test_size=TEST_SIZE, random_state=RANDOM_STATE, stratify=df_mature['event_observed']\n)\n\nprint(f\"‚úÖ Train/test split complete\")\nprint(f\"   - Train: {len(X_train):,} samples\")\nprint(f\"   - Test: {len(X_test):,} samples\")\nprint(f\"   - Features: {X_train.shape[1]}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 5. Create XGBoost DMatrix (GPU-optimized format)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Create DMatrix for XGBoost (GPU-optimized data structure)\nprint(\"Creating DMatrix for GPU training...\")\n\ndtrain = DMatrix(X_train, label_lower_bound=y_lower_train, label_upper_bound=y_upper_train, feature_names=feature_cols)\ndtest = DMatrix(X_test, label_lower_bound=y_lower_test, label_upper_bound=y_upper_test, feature_names=feature_cols)\n\nprint(\"‚úÖ DMatrix created (GPU-ready)\")\nprint(f\"   - Train DMatrix: {dtrain.num_row():,} rows x {dtrain.num_col()} features\")\nprint(f\"   - Test DMatrix: {dtest.num_row():,} rows x {dtest.num_col()} features\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 6. Quick Feature Importance (GPU-accelerated)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"üî• Training XGBoost for feature importance (100 trees on GPU)...\")\nstart_time = time.time()\n\n# Train with fewer trees for quick importance\nmodel_importance = xgb.train(\n    QUICK_XGBOOST_CONFIG,\n    dtrain,\n    num_boost_round=QUICK_XGBOOST_CONFIG['n_estimators'],\n    evals=[(dtrain, 'train'), (dtest, 'test')],\n    verbose_eval=False\n)\n\nelapsed = time.time() - start_time\n\nprint(f\"\\n‚úÖ Training complete in {elapsed:.1f}s (GPU-accelerated!)\")\nprint(f\"   Speed: {QUICK_XGBOOST_CONFIG['n_estimators'] / elapsed:.1f} trees/second\")\n\n# Get feature importance\nimportance_dict = model_importance.get_score(importance_type='gain')  # Use 'gain' for importance\nimportance_df = pd.DataFrame({\n    'feature': list(importance_dict.keys()),\n    'importance': list(importance_dict.values())\n}).sort_values('importance', ascending=False)\n\n# Normalize to percentages\nimportance_df['importance_pct'] = importance_df['importance'] / importance_df['importance'].sum() * 100\n\nprint(f\"\\nüìä Top 20 Features by Importance:\")\nprint(\"=\" * 60)\nfor idx, row in importance_df.head(20).iterrows():\n    print(f\"{row['feature']:40s} {row['importance_pct']:6.2f}%\")\n\n# Save\nimportance_df.to_csv(OUTPUT_PATH / 'feature_importance_xgboost.csv', index=False)\nprint(f\"\\n‚úÖ Saved to: {OUTPUT_PATH / 'feature_importance_xgboost.csv'}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"üî• Training Gradient Boosting Survival Analysis with ALL features...\")\n\ngbs_all = GradientBoostingSurvivalAnalysis(**GBS_CONFIG)\ngbs_all.fit(X_train_scaled, y_train)\n\n# Predict on test set\npred_gbs = gbs_all.predict(X_test_scaled)\n\n# Calculate C-index\nc_index_gbs = concordance_index_censored(\n    y_test['event'],  # Fixed\n    y_test['time'],   # Fixed\n    pred_gbs\n)[0]\n\nprint(f\"\\n‚úÖ Gradient Boosting (All Features)\")\nprint(f\"   - C-index: {c_index_gbs:.4f}\")\nprint(f\"   - Features used: {len(feature_cols)}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importance from RSF\n",
    "importances = rsf_all.feature_importances_\n",
    "\n",
    "# Create dataframe\n",
    "importance_df = pd.DataFrame({\n",
    "    'feature': feature_cols,\n",
    "    'importance': importances\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "# Normalize to percentages\n",
    "importance_df['importance_pct'] = importance_df['importance'] / importance_df['importance'].sum() * 100\n",
    "\n",
    "print(\"\\nüìä Top 20 Features by Importance:\")\n",
    "print(\"=\" * 60)\n",
    "for idx, row in importance_df.head(20).iterrows():\n",
    "    print(f\"{row['feature']:40s} {row['importance_pct']:6.2f}%\")\n",
    "\n",
    "# Save to CSV\n",
    "importance_df.to_csv(OUTPUT_PATH / 'feature_importance_all.csv', index=False)\n",
    "print(f\"\\n‚úÖ Saved to: {OUTPUT_PATH / 'feature_importance_all.csv'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize top 20 features\n",
    "plt.figure(figsize=(12, 8))\n",
    "top_20 = importance_df.head(20)\n",
    "plt.barh(range(len(top_20)), top_20['importance_pct'])\n",
    "plt.yticks(range(len(top_20)), top_20['feature'])\n",
    "plt.xlabel('Importance (%)')\n",
    "plt.title('Top 20 Features - Random Survival Forest')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_PATH / 'feature_importance_top20.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"‚úÖ Saved plot to: {OUTPUT_PATH / 'feature_importance_top20.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Feature Selection - Top K Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Test different numbers of top features\ntop_k_values = [5, 10, 15, 20, 30, 40, 50, len(feature_cols)]\n\nresults_top_k = []\n\nfor k in top_k_values:\n    print(f\"\\nüî• Testing with Top {k} features...\")\n    \n    # Select top k features\n    if k < len(feature_cols):\n        top_features = importance_df.head(k)['feature'].tolist()\n        feature_indices = [feature_cols.index(f) for f in top_features]\n        \n        X_train_k = X_train[:, feature_indices]\n        X_test_k = X_test[:, feature_indices]\n    else:\n        X_train_k = X_train\n        X_test_k = X_test\n        top_features = feature_cols\n    \n    # Train RSF\n    rsf_k = RandomSurvivalForest(**RSF_CONFIG)\n    rsf_k.fit(X_train_k, y_train)\n    \n    # Predict and evaluate\n    pred_k = rsf_k.predict(X_test_k)\n    c_index_k = concordance_index_censored(\n        y_test['event'],  # Fixed\n        y_test['time'],   # Fixed\n        pred_k\n    )[0]\n    \n    results_top_k.append({\n        'n_features': k,\n        'c_index': c_index_k,\n        'features': ', '.join(top_features[:5]) + ('...' if k > 5 else '')\n    })\n    \n    print(f\"   C-index: {c_index_k:.4f}\")\n    \n    # Memory cleanup\n    del rsf_k, pred_k\n    gc.collect()\n\n# Create results dataframe\ndf_top_k = pd.DataFrame(results_top_k)\n\nprint(\"\\nüìä Top-K Feature Selection Results:\")\nprint(\"=\" * 60)\nprint(df_top_k.to_string(index=False))\n\n# Save results\ndf_top_k.to_csv(OUTPUT_PATH / 'top_k_feature_results.csv', index=False)\nprint(f\"\\n‚úÖ Saved to: {OUTPUT_PATH / 'top_k_feature_results.csv'}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Top-K results\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(df_top_k['n_features'], df_top_k['c_index'], marker='o', linewidth=2, markersize=8)\n",
    "plt.xlabel('Number of Top Features')\n",
    "plt.ylabel('C-index')\n",
    "plt.title('Model Performance vs Number of Features')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.axhline(y=baseline_c_index, color='r', linestyle='--', label=f'Baseline (all features): {baseline_c_index:.4f}')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_PATH / 'top_k_performance.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"‚úÖ Saved plot to: {OUTPUT_PATH / 'top_k_performance.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Feature Group Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define feature groups based on extraction sections\n",
    "feature_groups = {\n",
    "    'Shannon Entropy': [c for c in feature_cols if 'entropy' in c.lower()],\n",
    "    'POI Counts': [c for c in feature_cols if '_count_' in c and c not in ['mosque_count_500m', 'mosque_count_1000m', 'pasar_count_1000m', 'convenience_count_1000m', 'spbu_count_2000m']],\n",
    "    'POI Densities': [c for c in feature_cols if '_density' in c and c not in ['transport_density', 'competition_density', 'street_food_density']],\n",
    "    'Indonesia Specific': ['mosque_count_500m', 'mosque_count_1000m', 'nearest_mosque_m', 'pasar_count_1000m', 'nearest_pasar_m', 'convenience_count_1000m', 'spbu_count_2000m', 'nearest_spbu_m', 'friday_prayer_multiplier', 'pasar_proximity_score', 'gas_proximity_score'],\n",
    "    'Competition': [c for c in feature_cols if 'competitor' in c.lower() or 'competition' in c.lower() or 'cannibalization' in c.lower()],\n",
    "    'Demographics': [c for c in feature_cols if 'income' in c or 'density_district' in c or 'working_age_district' in c],\n",
    "    'Accessibility': [c for c in feature_cols if 'dist_city_center' in c or 'transport_density' in c or 'centrality' in c],\n",
    "    'Interactions': [c for c in feature_cols if any(x in c for x in ['income_population', 'working_age_mall', 'office_transport', 'demand_supply', 'mosque_residential', 'pasar_transport'])],\n",
    "    'Temporal': [c for c in feature_cols if any(x in c for x in ['ramadan', 'weekend', 'gajian', 'school_holiday'])]\n",
    "}\n",
    "\n",
    "# Verify all features are categorized\n",
    "categorized = set()\n",
    "for group, features in feature_groups.items():\n",
    "    categorized.update(features)\n",
    "\n",
    "uncategorized = set(feature_cols) - categorized\n",
    "if uncategorized:\n",
    "    feature_groups['Other'] = list(uncategorized)\n",
    "\n",
    "print(\"üìä Feature Groups:\")\n",
    "print(\"=\" * 60)\n",
    "for group, features in feature_groups.items():\n",
    "    print(f\"{group:20s}: {len(features):2d} features\")\n",
    "    for f in features[:3]:\n",
    "        print(f\"  - {f}\")\n",
    "    if len(features) > 3:\n",
    "        print(f\"  ... and {len(features)-3} more\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Test each feature group independently\nresults_groups = []\n\nfor group_name, group_features in feature_groups.items():\n    if len(group_features) == 0:\n        continue\n    \n    print(f\"\\nüî• Testing '{group_name}' group ({len(group_features)} features)...\")\n    \n    # Get feature indices\n    feature_indices = [feature_cols.index(f) for f in group_features if f in feature_cols]\n    \n    if len(feature_indices) == 0:\n        print(\"   ‚ö†Ô∏è  No valid features found\")\n        continue\n    \n    X_train_g = X_train[:, feature_indices]\n    X_test_g = X_test[:, feature_indices]\n    \n    # Train RSF\n    rsf_g = RandomSurvivalForest(**RSF_CONFIG)\n    rsf_g.fit(X_train_g, y_train)\n    \n    # Predict and evaluate\n    pred_g = rsf_g.predict(X_test_g)\n    c_index_g = concordance_index_censored(\n        y_test['event'],  # Fixed\n        y_test['time'],   # Fixed\n        pred_g\n    )[0]\n    \n    results_groups.append({\n        'group': group_name,\n        'n_features': len(feature_indices),\n        'c_index': c_index_g\n    })\n    \n    print(f\"   C-index: {c_index_g:.4f}\")\n    \n    # Memory cleanup\n    del rsf_g, pred_g\n    gc.collect()\n\n# Create results dataframe\ndf_groups = pd.DataFrame(results_groups).sort_values('c_index', ascending=False)\n\nprint(\"\\nüìä Feature Group Performance:\")\nprint(\"=\" * 60)\nprint(df_groups.to_string(index=False))\n\n# Save results\ndf_groups.to_csv(OUTPUT_PATH / 'feature_group_results.csv', index=False)\nprint(f\"\\n‚úÖ Saved to: {OUTPUT_PATH / 'feature_group_results.csv'}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize group performance\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(range(len(df_groups)), df_groups['c_index'])\n",
    "plt.yticks(range(len(df_groups)), df_groups['group'])\n",
    "plt.xlabel('C-index')\n",
    "plt.title('Feature Group Performance')\n",
    "plt.axvline(x=baseline_c_index, color='r', linestyle='--', label=f'All features: {baseline_c_index:.4f}')\n",
    "plt.legend()\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_PATH / 'feature_group_performance.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"‚úÖ Saved plot to: {OUTPUT_PATH / 'feature_group_performance.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Progressive Feature Addition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Add feature groups progressively (by group performance)\nsorted_groups = df_groups.sort_values('c_index', ascending=False)['group'].tolist()\n\nresults_progressive = []\ncumulative_features = []\n\nfor i, group_name in enumerate(sorted_groups, 1):\n    # Add this group's features\n    group_features = feature_groups[group_name]\n    cumulative_features.extend([f for f in group_features if f in feature_cols])\n    \n    print(f\"\\nüî• Progressive Test {i}: Adding '{group_name}' ({len(cumulative_features)} total features)...\")\n    \n    # Get feature indices\n    feature_indices = [feature_cols.index(f) for f in cumulative_features]\n    \n    X_train_p = X_train[:, feature_indices]\n    X_test_p = X_test[:, feature_indices]\n    \n    # Train RSF\n    rsf_p = RandomSurvivalForest(**RSF_CONFIG)\n    rsf_p.fit(X_train_p, y_train)\n    \n    # Predict and evaluate\n    pred_p = rsf_p.predict(X_test_p)\n    c_index_p = concordance_index_censored(\n        y_test['event'],  # Fixed\n        y_test['time'],   # Fixed\n        pred_p\n    )[0]\n    \n    results_progressive.append({\n        'step': i,\n        'added_group': group_name,\n        'total_features': len(cumulative_features),\n        'c_index': c_index_p\n    })\n    \n    print(f\"   C-index: {c_index_p:.4f}\")\n    \n    # Memory cleanup\n    del rsf_p, pred_p\n    gc.collect()\n\n# Create results dataframe\ndf_progressive = pd.DataFrame(results_progressive)\n\nprint(\"\\nüìä Progressive Feature Addition:\")\nprint(\"=\" * 80)\nprint(df_progressive.to_string(index=False))\n\n# Save results\ndf_progressive.to_csv(OUTPUT_PATH / 'progressive_feature_results.csv', index=False)\nprint(f\"\\n‚úÖ Saved to: {OUTPUT_PATH / 'progressive_feature_results.csv'}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize progressive addition\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(df_progressive['step'], df_progressive['c_index'], marker='o', linewidth=2, markersize=8)\n",
    "plt.xlabel('Progressive Step')\n",
    "plt.ylabel('C-index')\n",
    "plt.title('Progressive Feature Group Addition')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xticks(df_progressive['step'], df_progressive['added_group'], rotation=45, ha='right')\n",
    "plt.axhline(y=baseline_c_index, color='r', linestyle='--', label=f'All features: {baseline_c_index:.4f}')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_PATH / 'progressive_addition.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"‚úÖ Saved plot to: {OUTPUT_PATH / 'progressive_addition.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Final Optimized Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify best configuration from experiments\n",
    "best_k = df_top_k.loc[df_top_k['c_index'].idxmax()]\n",
    "best_group = df_groups.iloc[0]\n",
    "best_progressive = df_progressive.loc[df_progressive['c_index'].idxmax()]\n",
    "\n",
    "print(\"üìä Best Configurations:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\n1. Top-K Features:\")\n",
    "print(f\"   - K = {best_k['n_features']}\")\n",
    "print(f\"   - C-index = {best_k['c_index']:.4f}\")\n",
    "\n",
    "print(f\"\\n2. Single Group:\")\n",
    "print(f\"   - Group = {best_group['group']}\")\n",
    "print(f\"   - Features = {best_group['n_features']}\")\n",
    "print(f\"   - C-index = {best_group['c_index']:.4f}\")\n",
    "\n",
    "print(f\"\\n3. Progressive Addition:\")\n",
    "print(f\"   - Step = {best_progressive['step']}\")\n",
    "print(f\"   - Last added = {best_progressive['added_group']}\")\n",
    "print(f\"   - Features = {best_progressive['total_features']}\")\n",
    "print(f\"   - C-index = {best_progressive['c_index']:.4f}\")\n",
    "\n",
    "print(f\"\\n4. All Features (Baseline):\")\n",
    "print(f\"   - Features = {len(feature_cols)}\")\n",
    "print(f\"   - C-index = {baseline_c_index:.4f}\")\n",
    "\n",
    "# Determine overall best\n",
    "all_scores = [\n",
    "    ('Top-K', best_k['c_index']),\n",
    "    ('Single Group', best_group['c_index']),\n",
    "    ('Progressive', best_progressive['c_index']),\n",
    "    ('All Features', baseline_c_index)\n",
    "]\n",
    "best_config = max(all_scores, key=lambda x: x[1])\n",
    "\n",
    "print(f\"\\nüèÜ BEST CONFIGURATION: {best_config[0]}\")\n",
    "print(f\"   C-index: {best_config[1]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Train final model with best configuration\n# Using Top-K if it performs best, otherwise all features\n\nif best_config[0] == 'Top-K' and best_k['n_features'] < len(feature_cols):\n    print(f\"\\nüî• Training final model with Top {best_k['n_features']} features...\")\n    top_features_final = importance_df.head(int(best_k['n_features']))['feature'].tolist()\n    feature_indices_final = [feature_cols.index(f) for f in top_features_final]\n    X_train_final = X_train[:, feature_indices_final]\n    X_test_final = X_test[:, feature_indices_final]\n    final_features = top_features_final\nelse:\n    print(f\"\\nüî• Training final model with ALL {len(feature_cols)} features...\")\n    X_train_final = X_train\n    X_test_final = X_test\n    final_features = feature_cols\n\n# Train with more trees for final model\nFINAL_RSF_CONFIG = RSF_CONFIG.copy()\nFINAL_RSF_CONFIG['n_estimators'] = 300\n\nrsf_final = RandomSurvivalForest(**FINAL_RSF_CONFIG)\nrsf_final.fit(X_train_final, y_train)\n\n# Predict on test set\npred_final = rsf_final.predict(X_test_final)\n\n# Calculate final C-index\nc_index_final = concordance_index_censored(\n    y_test['event'],  # Fixed\n    y_test['time'],   # Fixed\n    pred_final\n)[0]\n\nprint(f\"\\n‚úÖ FINAL MODEL PERFORMANCE\")\nprint(f\"   - C-index: {c_index_final:.4f}\")\nprint(f\"   - Features: {len(final_features)}\")\nprint(f\"   - Trees: {FINAL_RSF_CONFIG['n_estimators']}\")\nprint(f\"   - Max depth: {FINAL_RSF_CONFIG['max_depth']}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Summary Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive summary\n",
    "summary = {\n",
    "    'experiment': [\n",
    "        'All Features (RSF)',\n",
    "        'All Features (GBS)',\n",
    "        f'Top-{best_k[\"n_features\"]} Features',\n",
    "        f'Best Group ({best_group[\"group\"]})',\n",
    "        f'Progressive ({best_progressive[\"step\"]} groups)',\n",
    "        'Final Model (300 trees)'\n",
    "    ],\n",
    "    'n_features': [\n",
    "        len(feature_cols),\n",
    "        len(feature_cols),\n",
    "        int(best_k['n_features']),\n",
    "        int(best_group['n_features']),\n",
    "        int(best_progressive['total_features']),\n",
    "        len(final_features)\n",
    "    ],\n",
    "    'c_index': [\n",
    "        baseline_c_index,\n",
    "        c_index_gbs,\n",
    "        best_k['c_index'],\n",
    "        best_group['c_index'],\n",
    "        best_progressive['c_index'],\n",
    "        c_index_final\n",
    "    ]\n",
    "}\n",
    "\n",
    "df_summary = pd.DataFrame(summary)\n",
    "df_summary = df_summary.sort_values('c_index', ascending=False)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"üìä COMPREHENSIVE EXPERIMENT SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "print(df_summary.to_string(index=False))\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Save summary\n",
    "df_summary.to_csv(OUTPUT_PATH / 'experiment_summary.csv', index=False)\n",
    "print(f\"\\n‚úÖ Saved summary to: {OUTPUT_PATH / 'experiment_summary.csv'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# 1. Overall comparison\n",
    "ax1 = axes[0, 0]\n",
    "ax1.barh(range(len(df_summary)), df_summary['c_index'])\n",
    "ax1.set_yticks(range(len(df_summary)))\n",
    "ax1.set_yticklabels(df_summary['experiment'])\n",
    "ax1.set_xlabel('C-index')\n",
    "ax1.set_title('All Experiments Comparison')\n",
    "ax1.invert_yaxis()\n",
    "\n",
    "# 2. Top-K curve\n",
    "ax2 = axes[0, 1]\n",
    "ax2.plot(df_top_k['n_features'], df_top_k['c_index'], marker='o', linewidth=2)\n",
    "ax2.set_xlabel('Number of Features')\n",
    "ax2.set_ylabel('C-index')\n",
    "ax2.set_title('Top-K Feature Selection')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Feature group performance\n",
    "ax3 = axes[1, 0]\n",
    "ax3.barh(range(len(df_groups)), df_groups['c_index'])\n",
    "ax3.set_yticks(range(len(df_groups)))\n",
    "ax3.set_yticklabels(df_groups['group'])\n",
    "ax3.set_xlabel('C-index')\n",
    "ax3.set_title('Feature Group Performance')\n",
    "ax3.invert_yaxis()\n",
    "\n",
    "# 4. Progressive addition\n",
    "ax4 = axes[1, 1]\n",
    "ax4.plot(df_progressive['step'], df_progressive['c_index'], marker='o', linewidth=2)\n",
    "ax4.set_xlabel('Progressive Step')\n",
    "ax4.set_ylabel('C-index')\n",
    "ax4.set_title('Progressive Feature Addition')\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_PATH / 'comprehensive_summary.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"‚úÖ Saved comprehensive plot to: {OUTPUT_PATH / 'comprehensive_summary.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Key Findings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"üéØ KEY FINDINGS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\\n1. BEST OVERALL PERFORMANCE:\")\n",
    "print(f\"   - C-index: {c_index_final:.4f}\")\n",
    "print(f\"   - Configuration: {best_config[0]}\")\n",
    "print(f\"   - Features used: {len(final_features)}\")\n",
    "\n",
    "print(f\"\\n2. TOP 5 MOST IMPORTANT FEATURES:\")\n",
    "for idx, row in importance_df.head(5).iterrows():\n",
    "    print(f\"   {idx+1}. {row['feature']:40s} {row['importance_pct']:6.2f}%\")\n",
    "\n",
    "print(f\"\\n3. BEST FEATURE GROUP:\")\n",
    "print(f\"   - Group: {best_group['group']}\")\n",
    "print(f\"   - C-index: {best_group['c_index']:.4f}\")\n",
    "print(f\"   - Features: {best_group['n_features']}\")\n",
    "\n",
    "print(f\"\\n4. OPTIMAL NUMBER OF FEATURES:\")\n",
    "print(f\"   - Top-{best_k['n_features']} features achieve C-index {best_k['c_index']:.4f}\")\n",
    "if best_k['n_features'] < len(feature_cols):\n",
    "    improvement = (best_k['c_index'] - baseline_c_index) * 100\n",
    "    reduction = (1 - best_k['n_features']/len(feature_cols)) * 100\n",
    "    print(f\"   - Uses {reduction:.1f}% fewer features\")\n",
    "    print(f\"   - Performance change: {improvement:+.2f} percentage points\")\n",
    "\n",
    "print(f\"\\n5. MODEL COMPARISON:\")\n",
    "print(f\"   - Random Survival Forest: {baseline_c_index:.4f}\")\n",
    "print(f\"   - Gradient Boosting: {c_index_gbs:.4f}\")\n",
    "print(f\"   - Difference: {abs(baseline_c_index - c_index_gbs):.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Next Steps and Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"üí° RECOMMENDATIONS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\n1. FOR PRODUCTION DEPLOYMENT:\")\n",
    "if len(final_features) < len(feature_cols):\n",
    "    print(f\"   - Use Top-{len(final_features)} features (optimal balance)\")\n",
    "else:\n",
    "    print(f\"   - Use all {len(feature_cols)} features (best performance)\")\n",
    "print(f\"   - Model: Random Survival Forest with 300 trees\")\n",
    "print(f\"   - Expected C-index: {c_index_final:.4f}\")\n",
    "\n",
    "print(\"\\n2. FURTHER OPTIMIZATION:\")\n",
    "print(\"   - Hyperparameter tuning (GridSearchCV)\")\n",
    "print(\"   - Ensemble: Combine RSF + GBS predictions\")\n",
    "print(\"   - Feature engineering: Create more interactions from top features\")\n",
    "print(\"   - Cross-validation: Verify performance stability\")\n",
    "\n",
    "print(\"\\n3. DATA COLLECTION PRIORITIES:\")\n",
    "print(\"   Focus on collecting/improving:\")\n",
    "for idx, row in importance_df.head(3).iterrows():\n",
    "    print(f\"   - {row['feature']} ({row['importance_pct']:.1f}% importance)\")\n",
    "\n",
    "print(\"\\n4. MODEL MONITORING:\")\n",
    "print(\"   - Track C-index on new data\")\n",
    "print(\"   - Retrain quarterly with updated POI data\")\n",
    "print(\"   - Monitor feature importance drift\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"‚úÖ TRAINING COMPLETE - All results saved to:\", OUTPUT_PATH)\n",
    "print(\"=\" * 80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}